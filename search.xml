<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python command]]></title>
    <url>%2FAngelNI.github.io%2FPip-command%2F</url>
    <content type="text"><![CDATA[不是 ,有钱才善良，而是善良才富足。 pip升级pip show pippython -m pip install –upgrade pip 列出已安装的包 pip list pip安装包pip install 安装包名 pip查看是否已安装pip show [–files] 安装包名 pip检查哪些包需要更新pip list –outdated pip升级包pip install –upgrade 要升级的包名 pip卸载包pip uninstall 要卸载的包名 pip搜索包 pip search SomePackage pip参数解释 12345678910111213141516171819202122232425262728293031323334353637383940pip --helpUsage: pip &lt;command&gt; [options]Commands: install Install packages. download Download packages. uninstall Uninstall packages. freeze Output installed packages in requirements format. list List installed packages. show Show information about installed packages. check Verify installed packages have compatible dependencies. config Manage local and global configuration. search Search PyPI for packages. wheel Build wheels from your requirements. hash Compute hashes of package archives. completion A helper command used for command completion. help Show help for commands.General Options: -h, --help Show help. --isolated Run pip in an isolated mode, ignoring environment variables and user configuration. -v, --verbose Give more output. Option is additive, and can be used up to 3 times. -V, --version Show version and exit. -q, --quiet Give less output. Option is additive, and can be used up to 3 times (corresponding to WARNING, ERROR, and CRITICAL logging levels). --log &lt;path&gt; Path to a verbose appending log. --proxy &lt;proxy&gt; Specify a proxy in the form [user:passwd@]proxy.server:port. --retries &lt;retries&gt; Maximum number of retries each connection should attempt (default 5 times). --timeout &lt;sec&gt; Set the socket timeout (default 15 seconds). --exists-action &lt;action&gt; Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort. --trusted-host &lt;hostname&gt; Mark this host as trusted, even though it does not have valid or any HTTPS. --cert &lt;path&gt; Path to alternate CA bundle. --client-cert &lt;path&gt; Path to SSL client certificate, a single file containing the private key and the certificate in PEM format. --cache-dir &lt;dir&gt; Store the cache data in &lt;dir&gt;. --no-cache-dir Disable the cache. --disable-pip-version-check Don&apos;t periodically check PyPI to determine whether a new version of pip is available for download. Implied with --no-index. --no-color Suppress colored output 将pip源更换到国内镜像 123456常用的国内镜像包括：（1）阿里云 http://mirrors.aliyun.com/pypi/simple/（2）豆瓣http://pypi.douban.com/simple/（3）清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/（4）中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/（5）华中科技大学http://pypi.hustunique.com/ (1) 临时使用：可以在使用pip的时候，加上参数-i和镜像地址(如https://pypi.tuna.tsinghua.edu.cn/simple)。例如：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas，这样就会从清华镜像安装pandas库。 (2) 永久修改，一劳永逸： windows下，直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，然后新建文件pip.ini，即 %HOMEPATH%\pip\pip.ini，在pip.ini文件中输入以下内容（以豆瓣镜像为例）： 1234[global]index-url = http://pypi.douban.com/simple[install]trusted-host = pypi.douban.com]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network App Recommend]]></title>
    <url>%2FAngelNI.github.io%2FNetwork-App-Recommend%2F</url>
    <content type="text"><![CDATA[你知道吗，有些人，会以各种你情愿或不情愿的方式，留在你的记忆里，比如我。 Hosting PlatformGithub Gitlab Coding Bitbucket Bitballon 这五个比较常用的托管平台，当然了，这里，Github是全球最大的托管平台，我的博客也是在上面托管。但由于对国内限速，很是难受。其他的四个是我最近发现比较好用的，推荐给大家。 Cloud storageGoole云端硬盘 百度网盘 坚果云 Goole云端硬盘是我比较好用的，能关联许多应用，并且我的Colab的代码在上面，执行代码直接调用就可以了。百度网盘，是我第一个使用储存App，但是在下载上限速，非常_，不过幸好有百度云不限速的破解软件。坚果云，正尝试着去用。至于，其他的云储存，没怎末用过，要是用比较好的，留言推荐给我哦。 Search磁力猫 秘迹搜索 微软bing 鸠摩搜索 除了我们常用的百度谷歌搜狐等搜索引擎外，还有很多好玩的搜索。磁力猫是一个资源搜索引擎，在全网范围内搜索你想要的资源。秘迹搜索，是一个不追踪你位置的良心搜索。微软bing，嗯嗯。鸠摩搜索，是一个搜书的引擎，如果你喜欢看书，可是试试啊。 O JHDU POJ 51 Nod 牛客 洛谷 …… 对于一个计算机爱好者，ACM是最好的检验编程能力的平台，但不是每个人都可以参加ACM比赛的，现在有许多在线的OJ平台来练习你的算法编程能力，有好多，就不一一列举了。]]></content>
      <categories>
        <category>sharing</category>
      </categories>
      <tags>
        <tag>recommend</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Appreciation of Novels(2)]]></title>
    <url>%2FAngelNI.github.io%2FAppreciation-of-Novels-2%2F</url>
    <content type="text"><![CDATA[“不要忘记你曾经是怎样的小孩”，“不要忘记你曾希望变成怎样的大人”。 1.她对他很满意。走吧。好。他起身买单，腿却一拐一拐的。难怪他才华横溢，事业有成，却还是单身。趁着他买单，她赶紧悄悄走了。又是一年，她又遇到了他，他正牵着孩子的手，走的飞快。你的腿？她有些诧异。腿？我的腿怎么了？他更诧异。后来，她才知道他的腿，那天只是坐麻了而已。 2.失明后他脾气暴躁。妈妈呵斥道，你这样自暴自弃，从今后我只喊你起床吃饭睡觉，不再管你。果然，从那以后妈妈每天只跟他说这三句话。这让他很愧疚，也渐渐平静下来配合治疗。一年后，他终于复明了，却没看到妈妈。家人告诉他：妈妈一年前就去世了，去世之前录下那三句话，不想影响你的治疗… 3.她花了一周的晚上给他织好了这条围巾，从小娇生惯养，这是她的第一条围巾，她幻想着他惊喜的表情。在他生日的那个晚上，她刚幸福地把围巾给他围上，他却厌倦地取了下来“我不喜欢围巾”！心，瞬间冰凉！爸爸来了，以为是给自己的，自顾地围上，满脸都是幸福的笑容。她转过身来，泪流满面… 4.世界突然爆发一种健忘流行病。我和你都不幸被传染，并且越来越严重。第一天，我们都忘记带钥匙出门，于是只能半夜叫锁匠；第二天，一起做饭结果做出了咖喱牛排，其实我爱吃的是咖喱饭，而你爱菲力牛排；第三天，商场拒绝我的付款，因为我在信用卡的回执上，不管怎么回忆，都只签得出你的名字。 5.“今晚要开会，不用等我了.”“哦，知道了。”挂掉电话，她看着精心准备的一桌子菜发呆.总是忙，连我生日都忘了。门铃响了，“保安，有人看到你家阳台进了窃贼.”“啊?”她惊讶的看着保安鱼贯而入，很快听见阳台传来熟悉的声音：“谁是小偷?我是这房子的业主！喂干什么！别弄坏我蛋糕…” 6.他这一辈子都是默默无闻的在拍戏，演的永远是他的敌人，出镜率不高，并不出名。但他每天最开心的事情就是在公司看到他被粉丝里三层外三层的围住要求签名、合影，他总是微笑着站在一边静静等待，等他摆脱了粉丝走到自己身边对他说：走吧，迟到了导演会骂。据说，他叫奥特曼，他叫小怪兽。 7.儿子怀揣四万块冲进病房，对弥留的老父激动地大喊：“爸！我终于借到钱了！你可以动手术了！！”父亲嘴唇濡动。儿子问：“妈，爸在说什么？咱快叫医生啊！” 母逼近丈夫的脸颊，倾听片刻，对儿子泣道，“你爸想求你个事。你小时候，他常抱你，现在他要走了，你能不能抱一抱他？” 8.情人节，老年痴呆的外公失踪。晚间，医院来电说有位衣服上缝这个电话的老人站在某病房里不肯离去。去接外公时妈妈一进病房便哭了，外婆就是在这间病房去世的。当我看到傻傻的外公手里那支不知从哪里拣来的玫瑰时，忽然想到几年前情人节，我问外公咋不送外婆玫瑰时，外公说傻老太太衬不上玫瑰。 9.她车祸去世后，他思念万分，利用时光机回到过去，阻止惨剧发生。机器出了差错，比预定时间早了几分钟。他拿出钥匙开门，听见卧室传出她的娇喘和男人的声音。她手机响了，他记得这是他打来的。“我得走了，我男人催我呢。”他听着，惹羞成怒，出门偷了一辆车，看着急匆匆的她，一脚踩下油门… 10.这是他从医30年来第一起医疗事故，其实这种手术他做过不知道多少了.当患者死在手术台上的时候，他才意识到自己犯了多大的错误，这把曾经的“神刀”就此成为历史.回家后他异常疲惫，倒在沙发上一言不发.妻子很兴奋的冲出来，“女儿有救了，有个刚死的捐了肾。”“哦。”他握刀的手依旧在抖。 11.他在大街上遇见她，她带着孩子.他问：你还好吗。“挺好的，你呢。”“我也挺好的”，他摸摸小孩儿的头，软软的自来卷，“孩子真可爱，多大了？”“3岁.” 他沉默了一下，“原来我们分手那年你就结婚了.”她没说话，看着他的光头，他攥了下手里的化疗单“我那头自来卷太难打理，剃了.” 12.外人眼里，他们是可爱的龙凤胎。事实上，哥哥是克隆人，他不过是她的备用器官库，他这一生注定为她而活。十六岁那年，妹妹心脏出问题，这意味着哥哥的生命到了尽头。可她不愿意他替她去死，偷跑出去，晕死街头，他背她回来。等她醒来，他不在了，看到一张纸条：“放心，克隆人没有喜悲。” 13.为了庆祝分手后他的第一个生日，她低价卖掉了他往年送给她的每一样生日礼物。然后拿着卖来的钱她去了蛋糕房为他订了一份四层的大蛋糕，和一百根白色的生日蜡烛。他生日那天，随蛋糕一起寄给他的生日贺卡上她用红色的墨水一笔一划地写着：祝你孤独，并且长命百岁。 14.退休在家后，老伴最爱从早到晚数落我又老又胖好吃懒做。今早起床我突然咳嗽并吐出一口鲜血，他看到后整个上午没有说出一句话，闷闷地抽着烟。中午拉我去了医院，最后得知那是我牙龈发炎口腔出的血，他立马就站在医院怒骂我：“你这个没用的胖老太婆…”只是还没骂完，他眼眶里已满是泪水…]]></content>
      <categories>
        <category>literature</category>
      </categories>
      <tags>
        <tag>literature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NCF Data Processing]]></title>
    <url>%2FAngelNI.github.io%2FNCF-Data-Processing%2F</url>
    <content type="text"><![CDATA[左眼永远见不到右眼，只能陪她一起哭泣。 NCF数据处理是对论文neural_collaborative_filtering作者所提出的神经网络协同过滤源代码的运行结果，不过在源代码的基础上做了一些更改，运行环境是 python3.6，keras1.2.2，tensorflow1.3.0 ，电脑本地运行约7个小时。 数据处理前言 GMF batch_size=256, dataset=’ml-1m’, epochs=100, learner=’adam’, lr=0.001, num_factors=8, num_neg=4, out=1, path=’Data/‘, regs=’[0,0]’, verbose=1 MLP batch_size=256, dataset=’ml-1m’, epochs=100, layers=’[64,32,16,8]’, learner=’adam’, lr=0.001, num_neg=4, out=1, path=’Data/‘, reg_layers=’[0,0,0,0]’, verbose=1 NeuMF batch_size=256, dataset=’ml-1m’, epochs=100, layers=’[64,32,16,8]’, learner=’adam’, lr=0.001, mf_pretrain=’’, mlp_pretrain=’’, num_factors=8, num_neg=4, out=1, path=’Data/‘, reg_layers=’[0,0,0,0]’, reg_mf=0, verbose=1 #user=6040, #item=3706, #train=994169, #test=6040 评估 leave-one-out 命中率（HR） 归一化折扣累积增益（NDCG） 读取数据123456import pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineGMF = pd.read_table('GMF.txt',header=None, encoding='gb2312', sep=',')MLP = pd.read_table('MLP.txt',header=None, encoding='gb2312', sep=',')NeuMF = pd.read_table('NEUMF.txt',header=None, encoding='gb2312', sep=',') 查看数据 获取数据获取HR数据123456789GMF_HR = []MLP_HR = []NeuMF_HR = []for i in GMF[0]: GMF_HR.append(eval(i[-6:]))for i in MLP[0]: MLP_HR.append(eval(i[-6:]))for i in NeuMF[0]: NeuMF_HR.append(eval(i[-6:])) 获取NDGC123456789GMF_NDGC= []MLP_NDGC = []NeuMF_NDGC = []for i in GMF[1]: GMF_NDGC.append(eval(i[-6:]))for i in MLP[1]: MLP_NDGC.append(eval(i[-6:]))for i in NeuMF[1]: NeuMF_NDGC.append(eval(i[-6:])) 获取loss123456789GMF_loss= []MLP_loss = []NeuMF_loss = []for i in GMF[2]: GMF_loss.append(eval(i[-14:-8]))for i in MLP[2]: MLP_loss.append(eval(i[-14:-8]))for i in NeuMF[2]: NeuMF_loss.append(eval(i[-14:-8])) 图表表示HR对比123456789101112131415f=plt.figure(figsize=(12,10))plt.rcParams['font.sans-serif'] = ['SimHei']plt.title("GMF-MLP-NeuMF HR 对比图",fontsize = 20)plt.xlabel("Iteration",fontsize = 20)plt.ylabel("HR",fontsize = 20)plt.plot(range(100),GMF_HR,label = "GMF")#,linestyle='--')plt.plot(range(100),MLP_HR,label = "MLP")#,linestyle='-.')plt.plot(range(100),NeuMF_HR,label ="NeuMF")plt.scatter(98,0.6437,marker='^',color = 'black')plt.scatter(29,0.6763,marker='^',color = 'black')plt.scatter(35,0.6848,marker='^',color = 'black')plt.legend()plt.grid(c="w")plt.show() NDGC 对比1234567891011121314f=plt.figure(figsize=(12,10))plt.rcParams['font.sans-serif'] = ['SimHei']plt.title("GMF-MLP-NeuMF NDGC 对比图",fontsize = 20)plt.xlabel("Iteration",fontsize = 20)plt.ylabel("NDGC",fontsize = 20)plt.plot(range(100),GMF_NDGC,label = "GMF")plt.plot(range(100),MLP_NDGC,label = "MLP")plt.plot(range(100),NeuMF_NDGC,label ="NeuMF")plt.scatter(98,0.3749,marker='^',color = 'black')plt.scatter(29,0.3988,marker='^',color = 'black')plt.scatter(35,0.4095,marker='^',color = 'black')plt.legend()plt.grid(c="w")plt.show() LOSS 对比1234567891011f=plt.figure(figsize=(12,10))plt.rcParams['font.sans-serif'] = ['SimHei']plt.title("GMF-MLP-NeuMF loss 对比图",fontsize = 20)plt.xlabel("Iteration",fontsize = 20)plt.ylabel("Loss",fontsize = 20)plt.plot(range(100),GMF_loss,label = "GMF")plt.plot(range(100),MLP_loss,label = "MLP")plt.plot(range(100),NeuMF_loss,label ="NeuMF")plt.legend()plt.grid(c ="w")plt.show()]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradient Descent]]></title>
    <url>%2FAngelNI.github.io%2FGradient-Descent%2F</url>
    <content type="text"><![CDATA[最美的等待是，我们——未来可期。 场景引入梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。 我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！ Gradient Descent相关概念1.步长或学习效率(learning rare)：步长决定在梯度下降过程中，每一步沿梯度负方向前进的距离。 2.假设函数(hppothesis function)：也就是我们的模型学习到的函数 记为 h_θ(x) = θ0x0+θ1+x1+θ2x2+…=θTX 3.损失函数(loss function): 损失函数是用来评估模型h_θ(x)的好坏，通常用损失函数来度量拟合的程度，线性回归中损失函数通常为label和假设函数输出的差的平方。自己理解为（实际值-真实值）的平方。 损失函数梯度下降的基本过程就和下山的场景很类似。 首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释) 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？接下来，我们从微分开始讲起 微分看待微分的意义，可以有不同的角度，最常用的两种是： 函数图像中，某点的切线的斜率 函数的变化率 几个微分的例子： 梯度梯度实际上就是多变量微分的一般化。 下面这个例子： 算法过程1.先决条件：确认优化模型的假设函数h_θ(x)和损失函数J_(θ) 2.参数的初始化: 初始化假设函数的参数θ(注：θ是一个向量），算法中止距离ϵ以及步长α 3.确定当前位置的损失函数的梯度，对于θ_j,梯度如下 4.确定是否所有的θ_j,梯度下降的距离都小于ϵ，如果小于则算法中止，当前为最后结果，否则，则重复步骤（3） 5.更新所有的θ，对于θ_j（其更新的表达式如下 梯度下降的形式BGD、SGD、以及MBGD三种算法中文名分别为 批量梯度下降（Batch gradient descent） 批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新 优点：全局最优解；易于并行实现； 缺点：当样本数目很多时，训练过程会很慢。 随机梯度下降（Stochastic gradient descent） 随机梯度下降是通过每个样本来迭代更新一次， 如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 小批量梯度下降（Mini-batch gradient descent） 有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。MBGD在每次更新参数时使用b个样本（b一般为10） 不过都叫梯度下降算法，可见他们的核心是没有变的，变化的只是取训练集的方式，而梯度下降最核心的就是对函数求偏导，这个是在高等数学里有的。 Practice1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npfrom scipy import statsimport matplotlib.pyplot as plt#构造训练数据 h（x）x = np.arange(0.,10.,0.2)m = len(x)x0=np.full(m,1.0)train_data = np.vstack([x0,x]).T #通过矩阵变化得到测试集【x0，x1】y = 4*x+1+np.random.randn(m)#构造“标准”答案def BGD(alpha,loops,epsilon): ''' alpha:步长 loops:循环次数 epsilon:收敛精度 ''' count=0#loop次数 thata = np.random.randn(2)#随机thata向量初始的值也就是起点位置 err = np.zeros(2)#上次thata的值，初始化为0的向量 finish=0#完成标志位 result = [] while count&lt;loops: count+=1 #所有训练数据的期望更新一次thata sum = np.zeros(2)#初始化thata更次年总和 for i in range(m): cost = (np.dot(thata,train_data[i])-y[i])*train_data[i] sum+=cost thata = thata-alpha*sum result.append(np.linalg.norm(thata-err)) if np.linalg.norm(thata-err)&lt;epsilon:#判断是否收敛 finish = 1 break else: err=thata#没有则将当前thata向量赋值给err，作为下次判断参数 print (f'SGD结果:\tloop——counts： [%d]\tthata[%f,%f]'%(count,thata[0],thata[1])) return thata,resultif __name__=='__main__': result=[] thata,result=BGD(0.00009,10000,1e-4) slope,intercept,r_value,p_value,slope_std_error=stats.linregress(x,y) print(f'Stata结果:\tintercept(截距)：[%s]\tslope(斜率)：[%s]'%(intercept,slope)) for i in range(len(result)): plt.scatter(i,result[i]) #plt.plot(x,y,'k+') #plt.plot(x,thata[1]*x+thata[0],'r') plt.show() 结果如下]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means]]></title>
    <url>%2FAngelNI.github.io%2FK-means%2F</url>
    <content type="text"><![CDATA[天空在高又怎样，抬起脚尖就可以离太阳就更近一点。 聚类对于”监督学习”(supervised learning)，其训练样本是带有标记信息的，并且监督学习的目的是：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“无监督学习”(unsupervised learning)中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是”聚类”(clustering)。 聚类是一种无监督的学习，它将相似的对象归到同一簇中。聚类的方法几乎可以应用所有对象，簇内的对象越相似，聚类的效果就越好。K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。 聚类和分类最大的不同在于，分类的目标是事先已知的，而聚类则不一样，聚类事先不知道目标变量是什么，类别没有像分类那样被预先定义出来，所以，聚类有时也叫无监督学习。 聚类分析试图将相似的对象归入同一簇，将不相似的对象归为不同簇，那么，显然需要一种合适的相似度计算方法，我们已知的有很多相似度的计算方法，比如欧氏距离，余弦距离，汉明距离等。事实上，我们应该根据具体的应用来选取合适的相似度计算方法。 “聚类算法”试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。 图解 上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。 当需要将其划分为两个簇时，即 k=2 时： 当需要将其划分为四个簇时，即 k=4 时： 聚类方法 1.K-means 2.DBSCAN聚类 3.DBSCAN笑脸聚类 k-means (无监督)概念理解kmeans算法又名k均值算法。其算法思想大致为：先从样本集中随机选取 k 个样本作为簇中心，并计算所有样本与这 k 个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。 根据以上描述，我们大致可以猜测到实现kmeans算法的主要三点： （1）簇个数 k 的选择 （2）各个样本点到“簇中心”的距离 （3）根据新划分的簇，更新“簇中心” 前提准备（1）K值的选择 k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。 （2）距离的度量 距离的度量的方法有以下几种 1.有序性距离度量 1234（1）闵科夫斯基距离（2）欧式距离（3）曼哈顿距离（4）皮尔逊系数 2.无序属性距离度量 3.混合属性距离度量 算法步骤1、为中心向量c1, c2, …, ck初始化k个种子 2、分组: 12（1）将样本分配给距离其最近的中心向量（2）由这些样本构造不相交（ non-overlapping ）的聚类 3、确定中心: 用各个聚类的中心向量作为新的中心 4、重复分组和确定中心的步骤，直至算法收敛。 3、算法 k-means算法 输入：簇的数目k和包含n个对象的数据库。 输出：k个簇，使平方误差准则最小。 算法步骤： 123451.为每个聚类确定一个初始聚类中心，这样就有K 个初始聚类中心。2.将样本集中的样本按照最小距离原则分配到最邻近聚类3.使用每个聚类中的样本均值作为新的聚类中心。4.重复步骤2.3直到聚类中心不再变化。5.结束，得到K个聚类 伪代码 为避免运行时间过长，通常设置一个最大运行轮数或最小调整幅度阈值，若达到最大轮数或调整幅度小于阈值，则停止运行。 K-means算法分析1、k-means算法的性能分析 主要优点： 是解决聚类问题的一种经典算法，简单、快速。 对处理大数据集，该算法是相对可伸缩和高效率的。因为它的复杂度是0 (n k t ) , 其中, n 是所有对象的数目, k 是簇的数目, t 是迭代的次数。通常k &lt; &lt;n 且t &lt; &lt;n 。 当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。 主要缺点 (1)、在簇的平均值可被定义的情况下才能使用，这对于处理符号属性的数据不适用。 (2)、在 K-means 算法中 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适； (3)、在 K-means 算法中，首先需要根据初始聚类中心来确定一个初始划分，然后对初始划分进行优化。这个初始聚类中心的选择对聚类结果有较大的影响，一旦初始值选择的不好，可能无法得到有效的聚类结果； (4)、该算法需要不断地进行样本分类调整，不断地计算调整后的新的聚类中心，因此当数据量非常大时，算法的时间开销是非常大的； (5)、若簇中含有异常点，将导致均值偏离严重（即:对噪声和孤立点数据敏感）； (6)、不适用于发现非凸形状的簇或者大小差别很大的簇。 K-Means算法对于不同的初始值，可能会导致不同结果。解决方法： 121.多设置一些不同的初值，对比最后的运算结果）一直到结果趋于稳定结束，比较耗时和浪费资源2.很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。这也是 K-means 算法的一个不足。有的算法是通过类的自动合并和分裂，得到较为合理的类型数目 K. 2、k-means算法的改进方法——k-prototype算法 k-Prototype算法：可以对离散与数值属性两种混合的数据进行聚类，在k-prototype中定义了一个对数值与离散属性都计算的相异性度量标准。 K-Prototype算法是结合K-Means与K-modes算法，针对混合属性的，解决2个核心问题如下： 123451.度量具有混合属性的方法是，数值属性采用K-means方法得到P1，分类属性采用K-modes方法P2，那么D=P1+a*P2，a是权重，如果觉得分类属性重要，则增加a，否则减少a，a=0时即只有数值属性2.更新一个簇的中心的方法，方法是结合K-Means与K-modes的更新方法。3、k-means算法的改进方法——k-中心点算法 k-中心点算法：k -means算法对于孤立点是敏感的。为了解决这个问题，不采用簇中的平均值作为参照点，可以选用簇中位置最中心的对象，即中心点作为参照点。这样划分方法仍然是基于最小化所有对象与其参照点之间的相异度之和的原则来执行的。 实例 由上可以看出，第一次迭代后，总体平均误差值52.25~25.65，显著减小。由于在两次迭代中，簇中心不变，所以停止迭代过程，算法停止。 12345678910111213data = [ [0,2], [0,0], [1,0], [5,0], [5,2]]import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefor i in range(len(data)): plt.scatter(data[i][0],data[i][1],color='r')plt.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162data_n= np.mat(data)def center(data ,k): dim = np.shape(data)[1] cen_M = np.mat(np.zeros((k,dim))) for i in range(dim): minJ = min(data[:,i]) rangeJ = float(max(data[:,i])-minJ) #print() #print('\n') #print(minJ) cen_M[:,i] = np.mat(minJ + rangeJ * np.random.rand(k,1)) #print(data) return cen_M#center(data_n,k)def kmeans(data,k): m = np.shape(data)[0]#列的大小 classassment = np.mat(np.zeros((m,2))) centerpoint = center(data,k) Flag = True conut = 0 while Flag: Flag = False for i in range(m): mindis=np.inf ; minindex=-1 for j in range(k): disJ = np.linalg.norm(np.array(centerpoint[j,:])-np.array(data[i,:])) if disJ &lt; mindis: mindis = disJ; minindex = j; if classassment[i,0] !=minindex: Flag = True classassment[i,:] = minindex,mindis**2 #print(classassment) for cent in range(k): ptsInClust = data[np.nonzero(classassment[:,0].A==cent)[0]]#get all the point in this cluster centerpoint[cent,:] = np.mean(ptsInClust, axis=0)#get all the point in this cluster return centerpoint,classassment centerpoint,classassment=kmeans(data_n,k) def showCluster(dataSet, k, centroids, clusterAssment): ''' 数据可视化,只能画二维的图（若是三维的坐标图则直接返回1） ''' numSamples, dim = dataSet.shape mark = ['or', 'ob', 'og', 'ok','oy','om','oc', '^r', '+r', 'sr', 'dr', '&lt;r', 'pr'] # draw all samples for i in range(numSamples): markIndex = int(clusterAssment[i, 0]) plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex]) mark = ['Pr', 'Pb', 'Pg', 'Pk','Py','Pm','Pc','^b', '+b', 'sb', 'db', '&lt;b', 'pb'] # draw the centroids for i in range(k): plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize = 12) plt.show()showCluster(data_n,2,centerpoint,classassment)print(data_n) knn k-means 对比]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matrix Factorization For Recommendation System]]></title>
    <url>%2FAngelNI.github.io%2FMF%2F</url>
    <content type="text"><![CDATA[云是雨的梦，雨是云的前生。 Concept矩阵分解是将矩阵分解为数个矩阵的乘积，用矩阵分解做协同过滤是广泛使用的方法 常见的有三种： 1.三角分解法 2.QR分解法 3.奇异值分解法 Matrix Decomposition Method奇异值分解SVD原始的SVD又名奇异值分解，如果是用户评分矩阵，首先需要对缺失值进行简单的不全，比如用全局平均，然后用SVD进行分解 其中，R为原始的评分矩阵，维度是mn，U和V分贝是一个km和kn的正交矩阵，S为kk的对角矩阵，对角线上的每一个元素都是矩阵的奇异值。这种纯数学的方法计算量特别大，实际应用中的数据根本处理不了。Simon Funk的Funk-SVD方法解决了这个问题，思想很简单：直接通过训练集的观察值利用最小化RMSE学习P、Q矩阵，这就是机器学习的思想了。 SVD++SVD矩阵分解非常成功，有很多的迭代的方法，最有名的就是SVD++了。提SVD++之前，我们先看一个简单的BiasSVD： u 为训练集中所有记录的平均全局数 b_u 为用户的偏置项，表示用户的评分偏好 b_i 为物品的偏置项，表示物品的本身质量 如果将用户历史行为对用户评分预测影响考虑进来就是SVD++算法： SVD++的核心思想是把基于领域的itemCF算法用矩阵分解的方法实现，转换的方法是这样的： Others1.FM 2.隐式反馈矩阵分解 3.基于特征的矩阵分解 MF For Recommendation System对于推荐系统来说存在两大场景即评分预测（rating prediction）与Top-N推荐（item recommendation，item ranking）。评分预测场景主要用于评价网站，比如用户给自己看过的电影评多少分（MovieLens），或者用户给自己看过的书籍评价多少分（Douban）。其中矩阵分解技术主要应用于该场景。Top-N推荐场景主要用于购物网站或者一般拿不到显式评分信息的网站，即通过用户的隐式反馈信息来给用户推荐一个可能感兴趣的列表以供其参考。 有如下R（5，4）的打分矩阵：（“-”表示用户没有打分），其中打分矩阵R（n，m）是n行和m列，n表示user个数，m表示iten个数 那么，如何根据目前的矩阵R（5,4）如何对未打分的商品进行评分的预测（如何得到分值为0的用户的打分值）？ ——矩阵分解的思想可以解决这个问题，其实这种思想可以看作是有监督的机器学习问题（回归问题）。 矩阵R可以近似表示为P与Q的乘积：R（n,m）≈ P(n,K)*Q(K,m) 矩阵分解的过程中，将原始的评分矩阵分解成两个矩阵和的乘积： 矩阵P(n,K)表示n个user和K个特征之间的关系矩阵，这K个特征是一个中间变量，矩阵Q(K,m)的转置是矩阵Q(m,K)，矩阵Q(m,K)表示m个item和K个特征之间的关系矩阵，这里的K值是自己控制的，可以使用交叉验证的方法获得最佳的K值。为了得到近似的R(n,m)，必须求出矩阵P和Q，如何求它们呢？ 步骤 1.首先令 2.损失函数： 使用原始的评分矩阵与重新构建的评分矩阵之间的误差的平方作为损失函数。 如果R（i，j）已知，则R（i，j）的误差平方和为 最终，需要求解所有的非“-”项的损失之和最小值： 3.使用梯度下降法获得修正的p和q分量： 根据梯度方向更新变量 4.不停迭代直至算法最终收敛（直到sum（e^2）&lt;=阈值 加入正则项 1.第一步同上 2.在通常求解过程中，为了能够有较好的泛化能力，会在损失函数中加入正则项对参数进行约束 也就是 3.使用梯度下降法获得修正的p和q 4.不停迭代直至算法最终收敛（直到sum（e^2）&lt;=阈值 【预测】利用上述的过程，我们可以得到矩阵，这样便可以为用户 i 对商品 j 进行打分 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#导包import numpy as npimport matplotlib.pyplot as plt%matplotlib inline#参数设置alph = 0.00049step = 9000beta = 0.05K = 3# MFdef MF(r,p,q,alph,step,beta): result = [] count = 0 while(count&lt;step): count+=1 for i in range(len(r)): for j in range(len(r)): #构建损失函数 if r[i][j]&gt;0: eij = r[i][j] - np.dot(p[i,:],q[:,j]) for k in range(K): pd_p = -2*eij*q[k][j]+beta * p[i][k] pd_q = -2*eij*p[i][k]+beta * q[k][j] p[i][k] -= alph*pd_p q[k][j] -= alph*pd_q e = 0 for i in range(len(r)): for j in range(len(r)): if r[i][j]&gt;0 : eij = r[i][j] - np.dot(p[i,:],q[:,j]) e += eij**2 for n in range(K): e += (beta/2)*(p[i][k]**2+q[k][j]**2) result.append(e) # print(e) return p , q , result #原始矩阵r = [ [1,0,3,0,4], [0,2,1,4,0], [1,0,0,2,3], [2,0,1,0,0], [0,0,2,0,0]]r = np.array(r)print(f"输入矩阵为\n&#123;r&#125;")p = np.random.rand(5,K)q = np.random.rand(K,5)new_p,new_q,result = MF(r,p,q,alph,step,beta)print(f'输出矩阵为\n&#123;np.dot(new_p,new_q)&#125;' )plt.plot(range(len(result)),result) 最后结果如图]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>MF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gobang？]]></title>
    <url>%2FAngelNI.github.io%2FGobang%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[时间带着明显的恶意，缓缓在我的身上流逝 向大家介绍一款游戏，就是五子棋。 什么，五子棋？？？ 没错，就是高大上的五子棋，这是一个基于神经网络用Python写的小游戏五子棋，经过大量的训练，已经很优秀了呢！！！不知道你敢不敢与他战斗啊. Introduce to you a game, that is gobang.What, Gobang???Yes, it’s Gobang in Gaoda. It’s a small game written by Python based on neural network. After a lot of training, it’s already excellent!!! I wonder if you dare to fight him. Github项目地址]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gobang？]]></title>
    <url>%2FAngelNI.github.io%2F%E4%BA%94%E5%AD%90%E6%A3%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[时间带着明显的恶意，缓缓在我的身上流逝 向大家介绍一款游戏，就是五子棋。 什么，五子棋？？？ 没错，就是高大上的五子棋，这是一个基于神经网络用Python写的小游戏五子棋，经过大量的训练，已经很优秀了呢！！！不知道你敢不敢与他战斗啊. Introduce to you a game, that is gobang.What, Gobang???Yes, it’s Gobang in Gaoda. It’s a small game written by Python based on neural network. After a lot of training, it’s already excellent!!! I wonder if you dare to fight him. Github项目地址]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN--tensorflow--code-learning]]></title>
    <url>%2FAngelNI.github.io%2FCNN--tensorflow-code-learn%2F</url>
    <content type="text"><![CDATA[不知道未来如何变化，总有人相信童话。 这是以tensorflow为框架，写的关于MNIST数据识别的卷积神经网络的python代码，这个代码是自己一点一点把别人的代码打印到Calab，修改，运行，再修改，再运行，我是代码的生产者，也是代码的搬运工，哈哈~ 发到博客上，也很方便看，啊哈哈哈。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import tensorflow as tf import tensorflow.examples.tutorials.mnist.input_data as input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True) #下载并加载mnist数据x = tf.placeholder(tf.float32, [None, 784]) #输入的数据占位符y_actual = tf.placeholder(tf.float32, shape=[None, 10]) #输入的标签占位符#定义一个函数，用于初始化所有的权值 Wdef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#定义一个函数，用于初始化所有的偏置项 bdef bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) #定义一个函数，用于构建卷积层def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')#定义一个函数，用于构建池化层def max_pool(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')#定义一个函数，用于初始化所有的权值 Wdef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)#定义一个函数，用于初始化所有的偏置项 bdef bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) #定义一个函数，用于构建卷积层def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')#定义一个函数，用于构建池化层def max_pool(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')#构建网络x_image = tf.reshape(x, [-1,28,28,1]) #转换输入数据shape,以便于用于网络中W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) #第一个卷积层h_pool1 = max_pool(h_conv1) #第一个池化层W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) #第二个卷积层h_pool2 = max_pool(h_conv2) #第二个池化层W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) #reshape成向量h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) #第一个全连接层keep_prob = tf.placeholder("float") h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) #dropout层W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_predict=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) #softmax层cross_entropy = -tf.reduce_sum(y_actual*tf.log(y_predict)) #交叉熵train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(cross_entropy) #梯度下降法correct_prediction = tf.equal(tf.argmax(y_predict,1), tf.argmax(y_actual,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) #精确度计算sess=tf.InteractiveSession() sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: #训练100次，验证一次 train_acc = accuracy.eval(feed_dict=&#123;x:batch[0], y_actual: batch[1], keep_prob: 1.0&#125;) print('step %d, training accuracy %g'%(i,train_acc)) train_step.run(feed_dict=&#123;x: batch[0], y_actual: batch[1], keep_prob: 0.5&#125;)test_acc=accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_actual: mnist.test.labels, keep_prob: 1.0&#125;)print("test accuracy %g"%test_acc) 博文参考：https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-03-CNN1/]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KNN-Self-practice]]></title>
    <url>%2FAngelNI.github.io%2FKNN-Practice%2F</url>
    <content type="text"><![CDATA[有一天我结婚了，你一定要来哦，因为没有新娘，那该有多尴尬. 这是我的自己写的第一个KNN比较简单的练习案例，有关于KNN的介绍请参考我的上篇博文 This is my first simple exercise case of KNN written by myself. For an introduction to KNN, please refer to my last blog post. 我在这里 123456789101112131415161718192021222324252627#导包import numpy as npfrom matplotlib import pyplot as pltimport operator%matplotlib inline#自我创建数据集data=[ [0.8,1.8], [0.9,2.1], [1.0,1.5], [1.2,1.9], [1.3,2.0], [2.5,1.7], [2.8,1.5], [2.5,1.4], [2.7,1.9], [2.6,1.8], [1.9,3.3], [2.0,2.9], [2.2,2.8], [2.1,2.9], [1.8,3.0],]label=['a','a','a','a','a','b','b','b','b','b','c','c','c','c','c']print(label)train_data = np.array(data)print(train_data) 12345678910111213141516171819202122232425262728293031x_1=[]y_1=[]x_2=[]y_2=[]x_3=[]y_3=[]for i in range(5): x_1.append(data[i][0]) y_1.append(data[i][1]) x_2.append(data[i+5][0]) y_2.append(data[i+5][1]) x_3.append(data[i+10][0]) y_3.append(data[i+10][1])x = []y = []for j in range(15): x.append(data[j][0]) y.append(data[j][1])plt.scatter(x,y)print(f'&#123;x_1&#125; \n &#123;y_1&#125;\n&#123;x_2&#125; \n &#123;y_2&#125;\n&#123;x_3&#125;\n &#123;y_3&#125; ')f,ax=plt.subplots(1,1,figsize=(10,10))for i in range (5): ax.scatter(x_1[i],y_1[i],label='skitcat',color='r',marker='o') ax.scatter(x_2[i],y_2[i],label='skitcat',color='b',marker='o') ax.scatter(x_3[i],y_3[i],label='skitcat',color='g',marker='o')test = [[1.5,2.85]]##自定义点，从图中可以看出，很明显属于第三类x_test=1.5y_test=2.85ax.scatter(test[0][0],test[0][1],label='skitcat',color='m',marker = 'x')test = np.array(test) 从图中可以看出，很明显属于第三类 1234#定义距离公式def d_euc(x, y):#欧式距离 d = np.sqrt(np.sum(np.square(x- y))) return d 123456789101112131415161718192021def KNN(train_data,test,label,k): distance=[] for i in train_data: distance.append(d_euc(i,train_data)) distance = np.array(distance) index = distance.argsort() # 获取按距离大小排序后的索引 #print(index) sort_dis = np.sort(distance) count=&#123;&#125; o=0 print(label) for i in index: o=o+1 label_vote=label[i] count[label_vote] = count.get(label_vote,0)+1 ##返回特定的键值，否则返回 0 if o&gt;k: break print(label_vote) print(count) final_outcome=majory_vote(count) return final_outcome 12345#定义决策方案——多数表决法def majory_vote(count): sorted_class_count = sorted( count.items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count 12345label=['a','a','a','a','a','b','b','b','b','b','c','c','c','c','c']test=[3.0,2.0]test = np.array(test)final_label = KNN(train_data,test,label, 6)final_label 最后结果 ) 可以发现最初我们看到的真实结果一样属于c类。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KNN——k-Nearest Neighbor]]></title>
    <url>%2FAngelNI.github.io%2FKNN%E2%80%94%E2%80%94k-Nearest-Neighbor%2F</url>
    <content type="text"><![CDATA[海绵宝宝：“派大星，你为什么叫派大星” 派大星：“因为我是上帝拍下来保护你的大星星” 一、Concept1.1Language DescriptionK最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是所说的K个邻居）， 这K个实例的多数属于某个类，就把该输入实例分类到这个类中。 KNN 算法的核心思想和最近邻算法思想相似，都是通过寻找和未知样本相似的类别进行分类。但 NN 算法中只依赖 1 个样本进行决策，在分类时过于绝对，会造成分类效果差的情况，为解决 NN 算法的缺陷，KNN 算法采用 K 个相邻样本的方式共同决策未知样本的类别,这样在决策中容错率相对于 NN 算法就要高很多，分类效果也会更好。 1.2 graphic例子：要区分“猫”和“狗”，通过“claws”和“sound”两个feature来判断的话，圆形和三角形是已知分类的了，那么这个“star”代表的是哪一类呢？ k＝3时，这三条线链接的点就是最近的三个点，那么圆形多一些，所以这个star就是属于猫。 二、Algorithmic Description1.pseudo code 2.steps 初始化距离为最大值 计算未知样本和每个训练样本的距离dist 得到目前K个最邻近样本中的最大距离maxdist 如果dist小于maxdist，则将该训练样本作为K-最近邻样本 重复步骤2,3,4，直到未知样本和所有训练样本的距离都算完 统计K个最近邻样本中每个类别出现的次数 选择出现频率最大的类别作为未知样本的类别 三、KNN‘s three elements of model1.Distance measure距离度量，说白了就是距离计算公式。 常见的距离计算公式有如下： 1.欧氏距离 2.曼哈顿距离 3.余弦距离 4.皮尔逊系数 5.杰卡德距离 6.闵可夫斯基距离 7.切比雪夫距离 8.汉明距离 9.莱文斯坦距离1.1Euclidean distance欧氏距离是最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，是闵可夫斯基距离=2特殊情形. def d_euc(x, y): d = np.sqrt(np.sum(square(x - y))) return d1.2Manhattan Distance 12345def d_man(x, y): d = np.sum(abs(x - y)) return d 2.Selection of K Value不要小看了这个K值选择问题，因为它对K近邻算法的结果会产生重大影响。 1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。 3.Classification Decision Rules1.多数表决法 多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。 2.加权表决法 根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[map函数的简单用法]]></title>
    <url>%2FAngelNI.github.io%2Fmap%E5%87%BD%E6%95%B0%E7%9A%84%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[你要开心，你要快乐，因为你是大哥，不可以难过。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My First Neural Network Construction]]></title>
    <url>%2FAngelNI.github.io%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[啊啊啊Ｏ(≧口≦)Ｏ！！！我的第一个神经网络竟然是算出来的。 学习了简单的神经网络模型，今天出于兴趣，自己搭个神经网络的巨简单的模型，不不不，是算出来的。 这篇代码是根据我的上一篇博客点我吧根据推导公式写的，小白技能有限，大佬不要嘲笑啊。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import numpy as npimport matplotlib.pyplot as plt%matplotlib inline#激活函数及导数def sigmoid(x): return 1 / (1 + np.exp(-x))def d_sigmoid(x): return x * (1 - x) def tanh(x): return np.tanh(x)def d_tanh(x): return 1.0 - np.tanh(x) * np.tanh(x)##参数设置alph = 0.5esp = 0.01step = 5000#前向传播def qianxiangchuanbo(init,weight,b): #输入层——&gt;隐含层 neth=[]#神经元输入加权和 outh=[]#神经元输出 #隐含层---&gt;输出层 neto=[]#输出神经元 outo=[]#神经元输出 neth.append(weight[0]*init[0]+weight[1]*init[1]+b[0]) neth.append(weight[2]*init[0]+weight[3]*init[1]+b[0]) outh.append(sigmoid(neth[0])) outh.append(sigmoid(neth[1])) neto.append(weight[4]*outh[0]+weight[5]*outh[1]+b[1]) neto.append(weight[6]*outh[0]+weight[7]*outh[1]+b[1]) outo.append(sigmoid(neto[0])) outo.append(sigmoid(neto[1])) return neth,outh,neto,outodef fanxiangchuanbo(out,outo,outh): new_weight=[] q=[] #输如层——&gt;隐藏层 a1=(-(init[0]-outo[0])*outo[0]*(1-outo[0])) a2=(-(init[1]-outo[1])*outo[1]*(1-outo[1])) q.append(a1) q.append(a2) w1 = q[0]*weight[4]+q[1]*weight[6]*outh[0]*(1-outh[0])*init[0] w2 = q[0]*weight[4]+q[1]*weight[6]*outh[0]*(1-outh[0])*init[1] w3 = q[0]*weight[5]+q[1]*weight[7]*outh[1]*(1-outh[1])*init[0] w4 = q[0]*weight[5]+q[1]*weight[7]*outh[1]*(1-outh[1])*init[1] new_weight.append(w1) new_weight.append(w2) new_weight.append(w3) new_weight.append(w4) #输出层——&gt;隐藏层 w5 = -(out[0]-outo[0])*outo[0]*(1-outo[0])*outh[0] w6 = -(out[0]-outo[0])*outo[0]*(1-outo[0])*outh[1] w7 = -(out[1]-outo[1])*outo[1]*(1-outo[1])*outh[0] w8 = -(out[1]-outo[1])*outo[1]*(1-outo[1])*outh[1] new_weight.append(w5) new_weight.append(w6) new_weight.append(w7) new_weight.append(w8) return new_weight#输入集init = [0.05,0.10]#真实输出集out = [0.01,0.99]#权重weight = [0.15,0.20,0.25,0.30,0.40,0.45,0.50,0.55]#偏置项b=[0.35,0.60]count=0result = []while True: count=count+1 neth,outh,neto,outo = qianxiangchuanbo(init,weight,b) e=(abs(out[0]-outo[0])+abs(out[1]-outo[1])) result.append(e) gd_weight = fanxiangchuanbo(out,outo,outh) for i in range(len(weight)): weight[i]=weight[i]-alph*gd_weight[i] # if e&lt;esp: # break if count &gt; step: breakfor k in range(len(result)): plt.scatter(k,result[k])plt.show()print(weight)print(out)print(outo) 最后的运行结果如图 由于此代码只运行了5000次，可以看出与实际的差距还是很大的，如果感兴趣，你可以试试增大迭代次数，或者控制精度。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Artificial Neural Networks——Activation Funcation]]></title>
    <url>%2FAngelNI.github.io%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[有时候不敢去拥有，因为害怕失去，所以非常努力地去奋斗，让自己累，让自己不去想。 激活函数定义 所谓激活函数（Activation Function），就是在人工神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。 判定每个神经元的输出 通俗来说，激活函数一般是非线性函数，其作用是能够给神经网络加入一些非线性因素，使得神经网络可以更好地解决较为复杂的问题。 常见的激活函数 1.sigmoid 2.tanh 3.ReLu 4.ELU 5.PReLU 这里简单的对前三个进行介绍 1.sigmoid Sigmoid 函数的取值范围在 (0,1) 之间，单调连续，求导容易，一般用于二分类神经网络的输出层。 sigmoid函数图像如图 sigmoid函数求导 缺点： 123451.Sigmoid 函数饱和区范围广，容易造成梯度消失2.参数矩阵 W 的每个元素都会朝着同一个方向变化，同为正或同为负。这对于神经网络训练是不利的，所有的 W 都朝着同一符号方向变化会减小训练速度，增加模型训练时间。3.Sigmoid 函数包含 exp 指数运算，运算成本也比较大 2.tanh 图像如图 tanh 函数的取值范围在 (-1,1) 之间，单调连续，求导容易。 相比于 Sigmoid 函数，tanh 函数的优点主要有两个： 121.其一，收敛速度更快，如下图所示，tanh 函数线性区斜率较 Sigmoid 更大一些。在此区域内训练速度会更快。2.其二，tanh 函数输出均值为零，也就不存在 Sigmoid 函数中 dW 恒为正或者恒为负，从而影响训练速度的问题。 缺点： 1tanh 函数与 Sigmoid 函数一样，也存在饱和区梯度消失问题。其饱和区甚至比 Sigmoid 还要大一些，但不明显。 3.ReLu 优点： 12345671.没有饱和区，不存在梯度消失问题。2.没有复杂的指数运算，计算简单、效率提高。3.实际收敛速度较快，大约是 Sigmoid/tanh 的 6 倍。4.比 Sigmoid 更符合生物学神经激活机制。 缺点： 121. ReLU 的输出仍然是非零对称的，可能出现 dW 恒为正或者恒为负，从而影响训练速度。2. 当 x&lt;0 时，ReLU 输出总为零。该神经元输出为零，则反向传播时，权重、参数的梯度横为零，造成权重、参数永远不会更新，即造成神经元失效，形成了“死神经元”。 如何选择激活函数1234567891.首选 ReLU，速度快，但是要注意学习速率的调整，2.如果 ReLU 效果欠佳,尝试使用 Leaky ReLU、ELU 或 Maxout 等变种。3.可以尝试使用 tanh。4.Sigmoid 和 tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者概率值。其它情况下，减少 Sigmoid 的使用。5.在浅层神经网络中，选择使用哪种激励函数影响不大。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Artificial Neural Networks]]></title>
    <url>%2FAngelNI.github.io%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E6%A1%88%E4%BE%8B%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[现在终于理解我们高中数学老师说的话了，计算不行，说明数学不行，数学不行能力不行，数学好才是真正的好！！！哈哈哈。 这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,…,xn},输出也是一堆数据{y1,y2,y3,…,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。 在这里，通过对上图简单的案例进行数学推导,激活函数默认为sigmoid函数（注：神经网络的基础知识可以参考Poll的笔记：[Mechine Learning &amp; Algorithm] 神经网络基础） 一、前向传播1.输入层—-&gt;隐含层计算神经元的输入加权和 计算神经元 h1、h2 的输出 2.隐含层—-&gt;输出层计算输出神经元o1、o2的值 至此，前向传导传播结束。 二、反向传播1.计算总误差 2.隐藏层—–&gt;输出层权值更新 同理可求 权值跟新 3.隐藏层—-&gt;输出层权值更新 权值跟新 权值更新后测试数据，会发现数据误差变小许多 三、栗子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import numpy as np#激励函数与其偏导数def tanh(x): return np.tanh(x)def tanh_derivative(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1 / (1 + np.exp(-x))def logistic_derivative(x): return logistic(x) * (1 - logistic(x) )#神经网络模型class NeuralNetwork: def __init__(self, layers, activation='tanh'): if activation == 'Logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_derivative self.weights = [] for i in range(1, len(layers)-1): # [0,1) * 2 - 1 =&gt; [-1,1) =&gt; * 0.25 =&gt; [-0.25,0.25) 随机权值 self.weights.append( (2*np.random.random((layers[i-1] + 1, layers[i] + 1 ))-1 ) * 0.25 ) self.weights.append( (2*np.random.random((layers[i] + 1, layers[i+1] ))-1 ) * 0.25 ) # for i in range(0, len(layers)-1): # m = layers[i] # 第i层节点数 # n = layers[i+1] # 第i+1层节点数 # wm = m + 1 # wn = n + 1 # if i == len(layers)-2: # wn = n # weight = np.random.random((wm, wn)) * 2 - 1 # self.weights.append(0.25 * weight) #类比梯度下降 def fit(self, X, y, learning_rate=0.2, epochs = 10000): X = np.atleast_2d(X) # temp = np.ones([X.shape[0], X.shape[1]+1]) # temp[:,0:-1] = X # X = temp X = np.column_stack((X, np.ones(len(X)))) y = np.array(y) for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 正向计算 for l in range(len(self.weights)): a.append(self.activation( np.dot(a[l], self.weights[l])) ) # 反向传播 error = y[i] - a[-1] deltas = [error * self.activation_deriv(a[-1])] # starting backprobagation layerNum = len(a) - 2 for j in range(layerNum, 0, -1): # 倒数第二层开始 deltas.append(deltas[-1].dot(self.weights[j].T) * self.activation_deriv(a[j])) # deltas.append(deltas[-(layerNum+1-j)].dot(self.weights[j].T) * self.activation_deriv(a[j])) deltas.reverse() for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta) def predict(self, x): x = np.array(x) temp = np.ones(x.shape[0] + 1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return ann = NeuralNetwork([2, 2, 1], 'tanh')x = np.array([[0,0],[0,1],[1,0],[1,1]])y = np.array([0,1,1,0])nn.fit(x, y)for i in [[0,0],[0,1],[1,0],[1,1]]: print (i, nn.predict(i)) 这是我的运行结果]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WordCloud——A Beautiful Cloud of Words]]></title>
    <url>%2FAngelNI.github.io%2FWordCloud%E2%80%94%E2%80%94%E7%BE%8E%E4%B8%BD%E7%9A%84%E8%AF%8D%E4%BA%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[I miss you。 词云图 Word nephogramWordcloud 是Python第三方库中用于制作简单分词云图的第三方库，可以根据自己喜欢的颜色，喜欢的形状制作出美丽的词云图。 所谓的词云图，也叫文字云，是对文本中出现频率较高的“关键词”予以视觉化的展现，词云图过滤掉大量的低频低质的文本信息，使得浏览者只要一眼扫过文本就可领略文本的主旨。可以在每次的报告中迅速的找到核心词汇，掌握接下来发展的目的，方向。 实现快速生成词云图建立一个file.txt的文本文件，把你要统计的文章保存的这个文件中，运行如下的代码就可以看到词云图啦 1234567891011121314from os import pathfrom wordcloud import WordCloudimport matplotlib.pyplot as plt# Read the whole text.text = open('file.txt').read()# Generate a word cloud imagewordcloud = WordCloud().generate(text)# Display the generated image:# the matplotlib way:plt.imshow(wordcloud, interpolation='bilinear')plt.axis("off") 效果如图 自定义形状上面的词云图又丑有难看对不对，不要着急，这里可以自定义词云图的形状，自定义颜色。 在这里我的图片是一张心形 12345678910111213141516171819202122from os import pathfrom PIL import Imageimport numpy as npfrom wordcloud import WordCloudimport matplotlib.pyplot as pltd=path.dirname('E:\\study\\jupyter notebook')text=open(path.join(d,"constitution.txt")).read()alice_mask = np.array(Image.open(path.join(d, "2.jpg")))wordcloud=WordCloud(background_color="white",max_words=2000,mask=alice_mask)wordcloud.generate(text)wordcloud.to_file(path.join(d,"3.jpg"))# 步骤4-1：创建一个图表画布plt.figure(10)# 步骤4-2：设置图片plt.imshow(wordcloud, interpolation="bilinear")# 步骤4-3：取消图表x、y轴plt.axis("off")# 显示图片plt.show() 是不是很有趣呀！ 还有很多有趣的东西等你发现呢]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Just do it!]]></title>
    <url>%2FAngelNI.github.io%2FJust-do-it%2F</url>
    <content type="text"><![CDATA[放假了哈，啊哈哈哈啊哈哈~🤣🤣 今天，7月11日，考完了最后一科大学物理，结束了大一一年的学习生活，话说匆匆，也来不及挥手。 大一这一年，自我感觉收获还是蛮多的，参加过青年志愿者志愿活动❤️，去支过教❤️，参加过社团学习，学过硬件的一些东西，在宿舍焊过电路板😱，做过变压器（这个是基础，没啥高大上的），只是，后来没坚持下来，自我解释到毕竟不是学电气专业的😑😑。作为一个计算机专业的，学好编程才对嘛，现在简单的掌握了C语言和Python，入门级别，还有许多要深入。其实，在这个学期，一直想进算法协会ACM，第一次选拔也没如愿，还好在第二次选拔靠运气进了。兴致冲冲的准备好好学习算法，好好打比赛，拿个奖还能炫耀一下（白日做梦中ing🙃🙃），临期末的突转，跑去学AI😵，还好有数据挖掘的基础，入门的机器学习的算法还是能理解，能简单用Python实现 （学习笔记在GitHub我的仓库中有😎）。希望在这个暑期培训中，能有收获🙏🙏，之后的一年中好好学习AI，希望能有个比较好的成果。当然，算法也要学的，AI的老师也是我们下学期的数据结构的老师😮😮，算法，灵魂啊，除了书本知识，会实际实践才是最重要的。大学，自我学习才是最重要的。比如博客我真是一点不懂，只是按照百度上的教程，查找教程，向同学问，开始搭建，一开始，折腾了三四天，才看起来比较像样。还是比较有感触的，从没有到有，今天，我才能在这里 b b😂😂。 大一的一年至此结束了，还有两个月就大二了，好慌好慌真的好慌。 没有暑假，有了第一次，还差第二次吗？]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[New blog]]></title>
    <url>%2FAngelNI.github.io%2F%E7%BA%A6%E6%95%B0%E4%B8%AA%E6%95%B0%E5%AE%9A%E7%90%86%2Famazing%2F</url>
    <content type="text"><![CDATA[This a sad story！！ 😐😐😐 因为好奇，想为自己的电脑安装个双系统，也就是Windows 和 Linux 双系统并存，在安装过程中，误把Linux安装在D盘，导致D盘中之前所有保存的文件清空（原来想安装在F盘）呜呜~，想想也没什么可以丢弃了，就坚持把乌班图安装完毕，安装完还是有点成就感的，啊哈哈，尽管对Linux一点不懂，没事，自学大法好，都是小问题，💪💪。 祸兮，福之所倚，福兮，祸之所伏。虽然博客根目录也被删了，我又找到一个新的博客框架，简单，整洁，还是比较喜欢的一款，没错就是这个—— icarus。我又重头开始装饰博客，特别添加了Music这个一个页面，听听音乐放松心情，这还得多谢好兄弟王同学❤️，博客奉上🔊 赴京书生。一开始调用的网易云的音乐外链，因为版权的问题（没钱没版权🙃）有些音乐播放不了，很是难受，只好找不收费的。现在这个是QQ音乐的，有好多好听的歌而且还会自动更新音乐列表，优秀👍👍👍 。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning makes me happy]]></title>
    <url>%2FAngelNI.github.io%2FLearning-makes-me-happy%2F</url>
    <content type="text"><![CDATA[SharingThere are many interesting things waiting to be discovered by us . It’s never too late to learn. Don’t indulge yourself , don’t let yourself regret . 图灵机器人在线聊天这是一个基于图灵机器人和微信公众号相结合推出的微信在线聊天系统。 如果你对这个感兴趣这里有实现的操作 –&gt;&gt; It’s me 百度云下载 It’s me 提取码: ejuh 程序员的暖心话 to who?我们虽然不是浪漫的产生者，但我们可以做浪漫的搬运工。 项目：everywechat 功能：定时给朋友发送天气，提醒，每日一句，也可以智能自动回复好友，基于图灵机器人 项目地址：who is me QQ木马的简单实现，核心编程Finding from D的个人博客 QQ总是被盗怎么办？ 作为一个程序猿，对底层的代码还是了解一下子。只供参考，技术交流，后果自负。 百度云不限速下载器Baidu Netdisk Downloader是一款图形界面的百度网盘不限速下载器，支持 Windows、Linux、Mac。 在线工具，程序员的工具箱光说不能表达，附图一张 我在这呢！ python3 教程这个是最近发现的，里面有很多有关python的教程，还有一些实战项目。 没错又是我 经典技术书籍分享我又来喽]]></content>
      <categories>
        <category>sharing</category>
      </categories>
      <tags>
        <tag>sharing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CET4]]></title>
    <url>%2FAngelNI.github.io%2FCET4%2F</url>
    <content type="text"><![CDATA[COME ON !!! 很高兴有这次机会提前考四级，希望要过呀，好好学英语，下学期六级要过啊。 在这里简单介绍一下四级。 一 、四级题型1.作文（15%） 2.听力（35%） （1）听力对话（15%） （2）听力短文（20%） 3.阅读理解（35%） （1）词汇理解（5%） （2）长篇阅读（10%） （3）仔细阅读（20%） 4.汉译英（15%） 总分 ：710，及格线：425 二、考前准备1.词汇英语单词是学习英语的基础，基础不牢，地动山摇，好好买一本英语四级的词汇书很重要（我买的是新东方的），要背单词哦。现在有很多背单词的APP，比如百词斩，有道词典，我认为不靠谱，看着看着，就看别的了（都懂的吧），还有个人认为纸质的书拿起来有感觉。 2.听力最头疼的就是听力了，占了很大的分值，听力就多听吧。还有就是要知道他可能会考什么，猜。 3.阅读理解做阅读理解，一定要多做题，买一本近几年的四级考试卷子每周一套（整个上半年我们老师是这样要求的），找到自己的做题感觉和方法很重要。 4.翻译多背一些固定搭配，多背一些词组，还有就是单词~ 三、考试时间安排 注意：答题卡分为两张1和2，先写作文，听力，听力结束后开始收答题卡1。 最后要预祝考四级的小伙伴，考的全会，蒙的全对。]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[俄罗斯方块]]></title>
    <url>%2FAngelNI.github.io%2F%E4%BF%84%E7%BD%97%E6%96%AF%E6%96%B9%E5%9D%97%2F</url>
    <content type="text"><![CDATA[嘿！小伙伴们，还记得俄罗斯方块吗？想必每个人都玩过这个简单刺激的小游戏吧！ 在这里分享给大家一个在线的俄罗斯方块——&gt;&gt; 大爷！来玩呦 github项目地址 点我呦 百度云源代码下载 点我呦 提取码：wb6q]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数取模]]></title>
    <url>%2FAngelNI.github.io%2F%E5%A4%A7%E6%95%B0%E5%8F%96%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[THIS A SAD STORY…… 頑張って 介绍取模想必大家都知道，比如7%5=2，10%3=1，当然了，这还只是简单的取模，适用于不超过计算机整数范围。如果超过了该怎么办呢？ 今天，这里介绍的就是大数取模。 一般取模大数储存对一个相当大的数，C语言里的整形是无法储存的，在这里，我们用字符串储存。 取模方法模仿我们曾学过的竖式乘除法。 对于一个大数 A ，他从高到低的每一位乘以10再对mod取余，最后的结果就是取模的结果。 ` 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;#define MOD 100000007long divmod(char *ch,int a)&#123; long s = 0; for(int i=0;ch[i]!='\0';i++) &#123; s=(s*10+ch[i]-'0')%MOD; &#125; return s;&#125;int main()&#123; long a,b; char num[100000]; gets(num); a = strlen(num); b = divmod(num,a); printf("%ld",b); return 0; &#125; ` 快速幂取模对于一般的大数取模，时间复杂度太大，可能会TE了。对于指数型的大数取模问题，，快速幂取模就简单方便多了，在空间和时间上相对于一般取模都做了优化。 方法快速幂取模的方法基于离散数学或数论的一条公式推导引理。 积的取余等于取余的积的取余 在这个定义的基础上对指数型数据进行拆分以及合并，从而实现快速幂取模。 举个栗子求（3^40）mod 6。 (3^40)–&gt;(9^20)–&gt;(18^10) ……这样依次类推 这里指数幂是偶数，如果是奇数先乘在重复上述工作。 再这里有一个简单判断奇偶数的方法。附上代码 123456789101112#include&lt;stdio.h&gt;int main()&#123; int n; while(~scanf("%d",&amp;n))&#123; if(n&amp;1) printf("奇数"); else printf("偶数");&#125; return 0;&#125; 这里简单的运用了位运算。这比一般奇偶的判断高大上多了。活到老学到老。。。 实现12345678910111213long powermode(long a, long b, long mod)&#123; long ans = 1; while (b) &#123; if (b &amp; 1) &#123; ans = (ans * a) % mod; b--; &#125; b /= 2; a = a * a % mod; &#125; return ans;&#125; 补充对于大数取模还有欧拉函数（费马小定理），技巧去摸，这里不多介绍，后续会补上]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[约数个数定理]]></title>
    <url>%2FAngelNI.github.io%2F%E7%BA%A6%E6%95%B0%E4%B8%AA%E6%95%B0%E5%AE%9A%E7%90%86%2F</url>
    <content type="text"><![CDATA[自从那时起，它就有了特别的含义，既是与她的一种约定，也是自己出海航行的方向。one piece 是我的，我可是要成为海贼王的男人哈哈哈。 引言今天遇到了一个有意思的题，让你求一个范围内的约数的个数的最大值，求约数个，哈哈，这真是简单，1,2,3,4从头数不就行了吗，你看我说的对不对哈哈。 还有一种办法是通过约数个数定理，好，废话说得不少，那就进入正题~ 定理百度百科 对于一个大于1正整数n可以分解质因数 ： 约数个数定理——来自百度百科 内容对于一个大于1正整数n可以分解质因数： 则n的正约数的个数就是 。 其中a1、a2、a3…ak是p1、p2、p3，…pk的指数。 证明首先同上，n可以分解质因数：n=p1^a1×p2^a2×p3^a3…pk^ak, 由约数定义可知p1^a1的约数有:p1^0, p1^1, p1^2……p1^a1 ，共（a1+1）个;同理p2^a2的约数有（a2+1）个……pk^ak的约数有（ak+1）个。 故根据乘法原理：n的约数的个数就是(a1+1)(a2+1)(a3+1)…(ak+1)。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DFS_Practice]]></title>
    <url>%2FAngelNI.github.io%2FDFS-%E8%87%AA%E6%88%91%E7%BB%83%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[DFS——exercise.I learned DFS last month,I almost forgot how to use it,so that I can’t solve a problem in a practice competition. So I require to review it,and review carefully! Question在这里你有一个 6*3 的一个数组，每行有1 ， 2 ， 3 三个数，并且每行按照六种顺序分别排列。当每一行都取一个数时，求出6个数之和最大的值。 这里有一个非常笨的方法，就是用六重For循环，是不是很惊讶。没错，当我的小伙伴告诉我时,我的内心是WTF的。 这不是重点，重点是想通过这个简单的题练习一下DFS的思想。 这仅仅是个简单的开始， Code12345678910111213141516171819202122232425#include&lt;iostream&gt;using namespace std;int a[10][5];int res;void dfs(int x,int sum = 0)&#123; if(x&gt;5) &#123; //printf("%d",res); res = max(res,sum); return; &#125; dfs(x+1,sum+a[x][0]); //dfs(x+1,sum+a[x][1]); //dfs(x+1,sum+a[x][2]);&#125;int main（）&#123; for(int i=0;i&lt;6;i++) for(int j =0;j&lt;3;j++) cin&gt;&gt;a[i][j]; dfs(0); cout&lt;&lt;res&lt;&lt;endl; return 0; &#125; FIRST第一次我控制只输入第一列，输出结果为六。 仔细想想，这个跟递归求n的阶乘有异曲同工之妙，不断的调用自己递归，直到满足条件回归。 SECOND这次我输入了6*2的数据，并将每次的相加求和的结果打印出来，算了算一共64种，也就是说一共有64种组合方法。 THIRD最后我将所有的数据输入得到了正确的结果18。 LAST仔细想想还是挺有趣的，想想那个yong FOR循环写的，一共729种可能，想想就可怕. TE,TE,TE,TE……. DFS模板介绍DFS问题的解决有一个dfs的套用模板，自我感觉挺有用的，如果你有更好的办法，留评论呦！！！ 123456789101112131415void dfs(step)`&#123; if(num==end) &#123; /*do something*/ return ;&#125; /*尝试每一种可能，和遍历数组差不多*/ for(int i =0;i&lt;end;i++) &#123; do something; dfs(step+1); undo something; &#125;rerun 0;`&#125; 回溯问题Q这里拿棋盘问题举个栗子。 POJ1321 请点击这里 在一个给定形状的棋盘（形状可能是不规则的）上面摆放棋子，棋子没有区别。要求摆放时任意的两个棋子不能放在棋盘中的同一行或者同一列，请编程求解对于给定形状和大小的棋盘，摆放k个棋子的所有可行的摆放方案C。 INPUT 输入含有多组测试数据。每组数据的第一行是两个正整数，n k，用一个空格隔开，表示了将在一个n*n的矩阵内描述棋盘，以及摆放棋子的数目。 n &lt;= 8 , k &lt;= n当为-1 -1时表示输入结束。随后的n行描述了棋盘的形状：每行有n个字符，其中 # 表示棋盘区域， . 表示空白区域（数据保证不出现多余的空白行或者空白列）。 OUTPUT 对于每一组数据，给出一行输出，输出摆放的方案数目C （数据保证C&lt;2^31）。 SOLVE1234567891011121314151617181920212223int DFS(int x,int y) &#123; if(y&gt;=k） &#123; ans++; return 0; &#125; for(int i=x;i&lt;n;i++) &#123; for(int j=0;j&lt;n;j++) &#123; if(!visit[j]&amp;&amp;mp[i][j]=='#')//回溯 &#123; visit[j]=1; DFS(i+1,y+1); visit[j]=0; &#125; &#125; &#125; return 0; &#125;在这里menset（visit，0，sizeof（visit））； DFS过程中，你要退一步，就必然需要保存你走过每个点的所有信息，而在退一步的过程中，你需要从当前状态回到之前的状态，那么这步操作就是回溯，回溯是递归的时候一定会产生的很自然的操作，只不过大部分情况下不需要回溯。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fleury 算法]]></title>
    <url>%2FAngelNI.github.io%2FFleury-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Fleury 算法Fleury算法是从离散书上看到的，书上详细的写了算法的操作。在这里用主要用C语言实现。在这里隆重感谢曹老板的鼎力支持。膜拜~ 伪代码输入 ：欧拉图 任取 v0∈V(G),令P0 = v0 ，i = 0. 设 Pi = v0e0v1e1……eivi, 1如果E（G） - &#123;e1,e2……ei&#125;中没有与vi关联的边则计算停止；否则按下述条件从E（G） -&#123;e1，e2，……ei&#125;中任取一条边ei+1： （a） ei+1与vi相关联； （b） 除非无别的边可提供，否则ei+1不应该为Gi = G-{e1，e2，……ei}中的桥。 设ei+1=（vi,vi+1）,把ei+1,vi+1加入pi得到pi+1. 令i=i+1，返回2. 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#include&lt;stdio.h&gt;void fleury ();int Bridge (int m, int k);int eulertu[10000][10000] = 0;int V,E;int main ()&#123;printf(“顶点数：”);scanf(“%d”,&amp;V);printf(“边数：”);scanf(“%d”,&amp;E); //输入几个点，几条边int i,j;int m,n;int count;int o=1;printf(“输入有边的俩个点:\n”);for (i = 0; i &lt; E; i++)&#123;scanf("%d %d",&amp;m,&amp;n); //输入有边的俩个点 eulertu[m][n] = eulertu[n][m] = 1; &#125;for (i = 0; i &lt; V; i++) //判断是否为欧拉图 &#123; count = 0; for (j = 0; j &lt; V; j++) &#123; if (eulertu[i][j]==1) count++; &#125; if (count%2!=0) &#123; printf("不是欧拉图"); o=0; break; &#125; if(o==0) break; &#125;if(o==1)&#123; fleury(); printf("end");&#125; return 0;&#125;void fleury ()&#123;int a[10000] =&#123;0&#125;;int i,j;int k = 0;int bridge = 1;int t;printf(“%d—&gt;”,a[0]);for (i = 0; i &lt; E; i++)&#123;for (j = 0; j &lt; V; j++)&#123;if(eulertu[a[k]][j]==1)&#123;eulertu[a[k]][j] = eulertu[j][a[k]] = 0;if(Bridge(a[k],j))&#123;t = j;eulertu[a[k]][j] = eulertu[j][a[k]] = 1;&#125;else&#123;k++;a[k] = j;printf(“%d—&gt;”,j);bridge = 0;break;&#125; &#125; &#125; if (bridge) &#123; eulertu[a[k]][t] = eulertu[t][a[k]] = 0; k++; a[k] = t; printf("%d---&gt;",t); &#125; bridge = 1;&#125;&#125;int Bridge (int m, int k)&#123;int a[10000];int i = 0;int t = 0;int n = 0;int p = 0;for(i=0;i&lt;10000;i++)a[i]=-1;a[t] = m;for (t = 0; a[t] != -1; t++ )&#123;for (i = 0; i &lt; V; i++)&#123;if (eulertu[a[t]][i] == 1 &amp;&amp; i == k)return 0;if (eulertu[a[t]][i] == 1)&#123; p=0; while(a[p]==-1) &#123; p++; n++; a[n] = i; &#125; &#125; &#125;&#125;return 1;&#125; ​]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fermat's Last Theorem]]></title>
    <url>%2FAngelNI.github.io%2FFermat-s-Last-Theorem%2F</url>
    <content type="text"><![CDATA[QuestionDescription 对于输入的n,判断这个一个三元方程xn+yn=znx^n+y^n=z^nx**n+y**n=z**n是否有整数解 Input 单组输入 第一行一个整数TTT,代表输入的数据个数 接下来T行，每行一个正整数n。 1≤T≤100 1≤n≤100000 Output 输出T行，对于每个输入的n,如果有整数解输出”YES”,否则输出”NO”. Analyse费马大定理，又被称为“费马最后的定理”，由17世纪法国数学家皮耶·德·费玛提出。 他断言当整数n &gt;2时，关于x, y, z的方程 x^n + y^n = z^n 没有正整数解。 费马达定理的证明有一个非常巧妙的方法证明，自己去领悟精髓吧。 code12345678910111213141516#include&lt;bits/stdc++.h&gt;using namespace std;const int N = 1e6+100;typedef long long ll;int main()&#123;int n,t;cin&gt;&gt;t;for(int i=1;i&lt;=t;i++)&#123;cin&gt;&gt;n;if(n&lt;=2) cout&lt;&lt;"YES"&lt;&lt;endl;else cout&lt;&lt;"NO"&lt;&lt;endl;&#125;return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_Operation on Excel]]></title>
    <url>%2FAngelNI.github.io%2FPython-Operation-on-Excel%2F</url>
    <content type="text"><![CDATA[====This my mood now==== I recently solve the problem of Excel ‘ data , it is hard to process Excel data because of huge amount of it.However, there is a better tool to process–Python.I simply write 34 lines to operation on Excel’s data to count th number of the key word. Just for recording. code12345678910111213141516171819202122232425262728293031323334import xlrdimport xlwtimport reimport numpy as npimport pandas as pd#import openpyxldata = xlrd.open_workbook('C:\\Users\\hp\\Desktop\\数据信息3.xlsx')table = data.sheets()[0]ncols = table.col_values(12)a = len(ncols)pattern = re.compile(r'\d+')d=[]for i in range(1,a): #print(pattern.findall(ncols[i])) # print("\n") b = len(pattern.findall(ncols[i])) c=[] for j in range(0,b): if eval(pattern.findall(ncols[i])[j] )&lt; 10 : c.append(pattern.findall(ncols[i])[j]) #print(c) d.append(len(c)) #print(d) f = xlwt.Workbook() #创建工作簿sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True) #创建sheetfor i in range(0,len(d)): sheet1.write(i+1,0,d[i])f.save("C:\\Users\\hp\\Desktop\\2.xls")print("结束")]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A New Start]]></title>
    <url>%2FAngelNI.github.io%2FA-New-Start%2F</url>
    <content type="text"><![CDATA[This is a nice day!!! After half a day’s hard work, my blog is successfully included by baidu and google,and I add some new features to my blog for attracting more people to visit. As we all know,photo is a good way to look back our experience,so I specially add Photo to my blog.I change the background of my blog,making it not monotonous.The big change is that my blog language is changed to Engelish and I begin to write blog in English ,thanks to my classmate Uncle_drew’s idea,and thanks for his blog ,I learn a lot from it. Thank you @ https://cndrew.cn/ Lastly,I sincerely hope that I can insist writing blog .]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Poetry Appreciation]]></title>
    <url>%2FAngelNI.github.io%2FPoetry-Appreciation%2F</url>
    <content type="text"><![CDATA[I think poetry is a kind of life experience,a kind of inner sublimation.Cultivate sentiment and enrich oneself. 木兰花令——纳兰性德 人生若只如初见，何事秋风悲画扇?等闲变却故人心，却道故人心易变。 骊山语罢清宵半，泪雨零铃终不怨。何如薄幸锦衣郎，比翼连枝当日愿。 仓央嘉措《四》好多年了 你一直在我的伤口中幽居 我放下过天地 却从未放下过你 我生命中的千山万水 任你一一告别 世间事 除了生死 哪一件不是闲事 谁的隐私不被回光返照 殉葬的花朵开合有度 菩提的果实奏响了空山 告诉我 你藏在落叶下的那些脚印 暗示着多少祭日 专供我在法外逍遥 致橡树——舒婷 我如果爱你——绝不像攀援的凌霄花，借你的高枝炫耀自己； 我如果爱你——绝不学痴情的鸟儿，为绿荫重复单调的歌曲； 也不止像泉源，常年送来清凉的慰藉； 也不止像险峰，增加你的高度，衬托你的威仪。甚至日光，甚至春雨。 不，这些都还不够！ 我必须是你近旁的一株木棉，作为树的形象和你站在一起。 根，紧握在地下；叶，相触在云里。 每一阵风过，我们都互相致意，但没有人，听懂我们的言语。 你有你的铜枝铁干，像刀，像剑，也像戟；我有我红硕的花朵，像沉重的叹息，又像英勇的火炬。 我们分担寒潮、风雷、霹雳；我们共享雾霭、流岚、虹霓。 仿佛永远分离，却又终身相依。 这才是伟大的爱情，坚贞就在这里： 爱——不仅爱你伟岸的身躯，也爱你坚持的位置，足下的土地。 一棵开花的树——席慕蓉 如何让你遇见我 在我最美丽的时刻 为这 我已在佛前求了五百年 求佛让我们结一段尘缘 佛於是把我化做一棵树 长在你必经的路旁 阳光下 慎重地开满了花 朵朵都是我前世的盼望 当你走近 请你细听 那颤抖的叶 是我等待的热情 而当你终於无视地走过 在你身後落了一地的 朋友啊 那不是花瓣 那是我凋零的心 热爱生命——汪国真 我不去想是否能够成功 既然选择了远方 便只顾风雨兼程 我不去想能否赢得爱情 既然钟情于玫瑰 就勇敢地吐露真诚 我不去想身后会不会袭来寒风冷雨 既然目标是地平线 留给世界的只能是背影 我不去想未来是平坦还是泥泞 只要热爱生命 一切，都在意料之中]]></content>
      <categories>
        <category>literature</category>
      </categories>
      <tags>
        <tag>literature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTML学习]]></title>
    <url>%2FAngelNI.github.io%2FHTML%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[HTNL——BeginHyper Text Markup Language ，short for HTML ,is a standard markup language for creating web pages.The web pages which we usually scan are written by it.I want to learn a fewer about it because of writing blog and modifying my blog’s framework. I need it. The simply example1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;this is a title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;First level title&lt;/h1&gt; &lt;p&gt;paragraph&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; explanation 12345678&lt;!DOCTYPE html&gt; 声明HTML5文档&lt;html&gt; 元素是HTML页面的根元素&lt;head&gt; 元素包含了文档的元数据&lt;title&gt; 文档的标题&lt;body&gt; 可见的页面内容&lt;h1&gt; 一级标题&lt;p&gt; 段落&lt;meta charset = "utf-8"&gt; 声明编码utf-8 Others1.HTML link1&lt;a href="http://baidu.com"&gt;This is baidu link&lt;/a&gt; 2.picture1&lt;imag scr="(url)" width="258" height="39" /&gt; 3.lind feed1&lt;br/&gt; 4.level1&lt;hr&gt; 5.notes1&lt;!--notes--&gt; 6.bold1&lt;b&gt; bold &lt;/b&gt; I simply learned basics of HTML.Maybe it is very simply ,but it’s a new stduy.Come on !!!]]></content>
      <categories>
        <category>Learning</category>
      </categories>
      <tags>
        <tag>Learing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天生棋局(指针)]]></title>
    <url>%2FAngelNI.github.io%2F%E5%A4%A9%E7%94%9F%E6%A3%8B%E5%B1%80-%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[上次用数组写的天生棋局题，这里补上指针版的。 题目描述中国传统文化源远流长，博大精深，包含着华夏先哲的无穷智慧，也是历朝历代炎黄子孙生活的缩影。围棋作为中华民族流传已久的一种策略性棋牌游戏，蕴含着丰富的汉民族文化内涵，是中国文明与中华文化的体现。本案例要求创建一个棋盘，在棋盘生成的同时初始化棋盘，根据初始化后棋盘中棋子的位置来判断此时的棋局是否是一局好棋。具体要求如下：** 1）棋盘的大小根据用户的指令确定； 2）棋盘中棋子的数量也由用户设定； 3）棋子的位置由随机数函数随机确定，若生成的棋盘中有两颗棋子落在同一行或同一列，则判定为“好棋”，否则判定为“不是好棋”。 #注释天生棋局指针类型的和上次数组类型的大体思路是一样的，在这里主要不同的，在于用calloc（）函数申请一个动态的存储空间，因为calloc（）函数成功生成动态存储空间会返回储存空间的首地址，所以在这里用指针类型的变量来实现对动态存储空间的操作。 这里主要用二维指针，二维指针储存一维指针的地址，二维指针可以看做二维数组，而二维数组可以看做由一维数组组生成，这样理解起来比较简单些 附上关键自定义生成动态存储函数 123456789 int ** board(int n)&#123; int ** p = (int **)calloc(sizeof(int*), n);//calloc在内存中分配n*size大小的动态存储空间，返回一个起始地址的一个指针 for (int i = 0; i &lt; n; i++) &#123; p[i] = (int *)calloc(sizeof(int), n); &#125; return p;&#125; 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;time.h&gt;int ** board(int n); //申请动态存储空间 void inkey(int **p,int n,int m);//用随机数下棋 void printboard(int **p,int n);//打印棋格 int check(int **p,int n);//检查好/坏棋 void freespace(int **p,int n);//释放动态空间 int main()&#123; int n,m,con; printf("请输入棋盘大小:\n"); scanf("%d",&amp;n); printf("请输入棋子数量:\n"); scanf("%d",&amp;m); int **p=board(n); inkey( p ,n, m ); printboard(p,n); con=check(p,n); freespace(p,n); if(con) printf("好棋！"); else printf("不是好棋！"); return 0; &#125; int ** board(int n)&#123; int ** p = (int **)calloc(sizeof(int*), n);//calloc在内存中分配n*size大小的动态存储空间，返回一个起始地址的一个指针 for (int i = 0; i &lt; n; i++) &#123; p[i] = (int *)calloc(sizeof(int), n); &#125; return p;&#125;void inkey(int **p,int n,int m)&#123; srand((unsigned int)time(NULL)); //随机数种子 生成伪随机数，每次的随机数都不一样 while(m--) &#123; int a=rand()%n,b=rand()%n; p[a][b]=1; &#125;&#125;void printboard(int **p,int n)&#123; for(int i=0;i&lt;n;i++) // 生 成 棋 盘 &#123; for(int j =0;j&lt;n;j++) // ┏ ┓┗ ┛┠ ┷ ┨ ┯ ┼ ● &#123; if(p[i][j]==1) printf("●"); else &#123; if (i == 0 &amp;&amp; j == 0) printf("┏"); else if (i == 0 &amp;&amp; j == n - 1) printf("┓"); else if (i == n - 1 &amp;&amp; j == 0) printf("┗"); else if (i == n - 1 &amp;&amp; j == n - 1) printf("┛"); else if (j == 0) printf("┠"); else if (i == n - 1) printf("┷"); else if (j == n - 1) printf("┨"); else if (i == 0) printf("┯"); else printf("┼"); &#125; &#125; putchar('\n'); &#125; &#125;int check(int **p,int n)&#123; int flag = 0; // 默认不是好棋。 for(int i=0;i&lt;n;i++) // 判断 好棋 坏棋 &#123; for(int j=0;j&lt;n;j++) &#123; if (p[i][j] == 1) &#123; if (j&gt;0 &amp;&amp; p[i][j-1] == 1) //判断同一行有无相邻棋子 &#123; flag = 1; &#125; if (i &gt;0 &amp;&amp; p[i-1][j] == 1) //判断同一列有无相邻棋子 &#123; flag = 1; &#125; &#125; &#125; &#125; return flag;&#125;void freespace(int **p,int n)&#123; for (int i= 0; i &lt; n; i++) &#123; free(p[i]); //释放一级指针指向的空间 &#125; free(p); //释放二级指针指向的空间&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sort() function]]></title>
    <url>%2FAngelNI.github.io%2Fsort-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[C++中的sort（）函数我在之前的博客中提到，解决排序问题的一个好用的函数就是C++的sort（）函数啦。sort（）函数是C++内置的函数，只需要加入头文件，掌握正确的使用方法，你就可以在排序中驰骋疆场了（自吹自擂）。好啦，下面就请主角登场吧 sort()1.介绍c++语言中 STL 库中的sort函数可以用来对数组进行排序。对于c++语言来说由于其自带的sort()函数更容易被编译器编译，其排序速度比基于快速排序的qsort要快上不少，且用法简单。(百度知道) 2.准备sort（）函数的使用需要添加头文件 123#include&lt;algorithm&gt;或者万能头文件#include&lt;bits/stdc++.h&gt; 3.使用方法sort（star,end,cmp）* sort函数有三个参数： 1.第一个是要排序的数组的起始地址 2.第二个是结束地址（最后一位的地址的下一地址） 3.第三个参数是排序的方法。sort函数默认是按从小到大排序。可以修改cmp实现从大到小排序 123sort（begin，end，less&lt;data-type&gt;)——升序sort（begin，end，greater&lt;data-type&gt;)——降序 以上是比较简单常用的对数组的排序方法，sort（）类函数中还有其他的排序功能。 4.sort()类函数 函数名 功能描述 sort 对给定区间所有元素进行排序 stable_sort 对给定区间所有元素进行稳定排序 partial_sort 对给定区间所有元素进行稳定排序 partial_sort 对给定区间所有元素部分排序 partial_sort_copy 对给定区间复制并排序 nth_element 找出给定区间的某个位置对应的元素 is_sorted 判断一个区间是否已经排好序 partition 使得符合某个条件的元素放在前面 stable_partition 相对稳定的使得符合某个条件的元素放在前面 5.sort（）函数练习1.有序序列合并链接： https://ac.nowcoder.com/acm/contest/827/J 来源：牛客网 题目描述 输入两个升序排列的序列，将两个序列合并为一个有序序列并输出。 输入描述: 1234567输入包含三行，第一行包含两个正整数n, m（1 ≤ n,m ≤ 100），用空格分隔。n表示第二行第一个升序序列中数字的个数，m表示第三行第二个升序序列中数字的个数。第二行包含n个整数（范围1~5000），用空格分隔。第三行包含m个整数（范围1~5000），用空格分隔。 输出描述: 输出为一行，输出长度为n+m的升序序列，即长度为n的升序序列和长度为m的升序序列中的元素重新进行升序序列排列合并。 示例1 输入5 61 3 7 9 222 8 10 17 33 44输出1 2 3 7 8 9 10 17 22 33 44 1234567891011121314151617181920212223242526272829#include&lt;cstdio&gt;#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;int n,m,t;int a[100],b[100],c[200];int main()&#123; cin&gt;&gt;n&gt;&gt;m; t=n; for(int i=0;i&lt;n;i++) &#123; cin&gt;&gt;a[i]; c[i]=a[i]; &#125; for(int i=0;i&lt;m;i++) &#123; cin&gt;&gt;b[i]; c[t++]=b[i]; &#125; sort(c,c+m+n); for(int i=0;i&lt;m+n;i++) &#123; cout&lt;&lt;c[i]; putchar(' '); &#125; return 0; &#125; 2.最高最低分差链接： https://ac.nowcoder.com/acm/contest/827/E 来源：牛客网 题目描述 输入n个成绩，换行输出n个成绩中最高分数和最低分数的差。 输入描述: 123两行，第一行为n，表示n个成绩，不会大于10000。第二行为n个成绩（整数表示，范围0~100），以空格隔开。 输出描述: 1一行，输出n个成绩中最高分数和最低分数的差。 示例1 输入 121098 100 99 97 95 99 98 97 96 100 输出 5 12345678910111213141516#include&lt;stdio.h&gt;#include&lt;algorithm&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; int n,a[10000],sum=0; scanf("%d",&amp;n); for(int i=0;i&lt;n;i++) scanf("%d",&amp;a[i]); sort(a,a+n); sum=a[n-1]-a[0]; printf("%d",sum); return 0; &#125; ###]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DFS]]></title>
    <url>%2FAngelNI.github.io%2FDFS%2F</url>
    <content type="text"><![CDATA[DFS深度优先搜索算法（Depth-First-Search），是搜索算法的一种。是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。当节点v的所有边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止（属于盲目搜索）。 “一路走到头，不撞墙不回头” 深度优先搜索属于图算法的一种，是一个针对图和树的遍历算法，英文缩写为DFS即Depth First Search。深度优先搜索是图论中的经典算法，利用深度优先搜索算法可以产生目标图的相应拓扑排序表，利用拓扑排序表可以方便的解决很多相关的图论问题，如最大路径问题等等。一般用堆数据结构来辅助实现DFS算法。其过程简要来说是对每一个可能的分支路径深入到不能再深入为止，而且每个节点只能访问一次。 树状图图解例如，想要从1到9，每到一个岔路口你有两种选择，你可以选择左枝，或者右枝，共两种可能，但是当你走到死胡同时，你只能原路返回，走到这个死胡同的上一个路口，走另一条路，依次类推，直到走到终点，也就是九。你可能会问，这不明摆着呢吗，直接从1经过8到9不就行了。没错，这是最直接的办法，但计算机傻啊，没有你聪明啊，它只会，一次一次的尝试，直到最终结果。 下面是图解 例题给定整数a1、a2、…….an，判断是否可以从中选出若干数，使它们的和恰好为K。 输入 首先，n和k，n表示数的个数，k表示数的和。接着一行n个数。（1&lt;=n&lt;=20,保证不超int范围） 输出 如果和恰好可以为k，输出“YES”，否则“NO” 样例输入 1234 131 2 4 7 样例输出 1YES 思路每一个数有加与不加两种可能，从树的一枝不加到尾，然后，再从叶末返回上一层叶节点，走另一个分支，也就是加上最后一个，与所求的和比较，不符再重复上述操作。直到找到与所求和相等返回Yes 实现1234567891011121314151617181920212223242526#include&lt;iostream&gt;#include&lt;stdio.h&gt;using namespace std;int n,k,a[50];int dfs(int i,int sum)&#123; if(i==n) return sum==k; if(dfs(i+1,sum)) return 1; if(dfs(i+1,sum+=a[i])) return 1; return 0; &#125;int main()&#123; cin&gt;&gt;n&gt;&gt;k; for(int i=0;i&lt;n;i++) cin&gt;&gt;a[i]; if(dfs(0,0)) cout&lt;&lt;"YES"&lt;&lt;endl; else cout&lt;&lt;"No!"&lt;&lt;endl; return 0; &#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[天生棋局]]></title>
    <url>%2FAngelNI.github.io%2F%E5%A4%A9%E7%94%9F%E6%A3%8B%E5%B1%80%2F</url>
    <content type="text"><![CDATA[天生棋局问题，是C语言老师留的一个课外练习题。要求有用指针来写，因为指针学的不扎实，也因为第一次看到这道题时，第一个想法就是要用数组来写，所以以下是用数组对天生棋局代码。(指针的会后续补上) 题目描述中国传统文化源远流长，博大精深，包含着华夏先哲的无穷智慧，也是历朝历代炎黄子孙生活的缩影。围棋作为中华民族流传已久的一种策略性棋牌游戏，蕴含着丰富的汉民族文化内涵，是中国文明与中华文化的体现。本案例要求创建一个棋盘，在棋盘生成的同时初始化棋盘，根据初始化后棋盘中棋子的位置来判断此时的棋局是否是一局好棋。具体要求如下：** 1）棋盘的大小根据用户的指令确定； 2）棋盘中棋子的数量也由用户设定； 3）棋子的位置由随机数函数随机确定，若生成的棋盘中有两颗棋子落在同一行或同一列，则判定为“好棋”，否则判定为“不是好棋”。 题前注释1.随机数头文件： 1#include&lt;stdlib.h&gt; And #include&lt;time.h&gt; 函数： 1.rand()函数生成伪随机数。 2.随机发生器的初始化函数`srand(unsigned seed) 目的： rand（）函数是按指定的顺序来产生整数，但可能两次的随机数相同并不是真正的随机，叫做伪随机数。而随机发生器的初始化函数`srand(unsigned seed)（也位于stdlib.h）进行伪随机数列初始化，通过用时间函数time（NULL）作为seed，使每一次产生的随机数都不一样。 2.棋盘，棋子这是一个下棋的游戏，如果把随机的产生的棋子赤果果地展现在棋盘上，效果会很明显，并且题目说要生成棋盘，所以首先要打印一个棋盘 打印棋盘，首先要有边框和棋子，这些是从word上copy来的,然后用双层循环就可以了。 3.判断好/坏棋根据题意即可 疑惑根据题意，他说若生成的棋盘中有两颗棋子落在同一行或同一列，则判定为“好棋”，否则判定为“不是好棋”。但与查的资料不同，说是两颗棋子相邻是好棋。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;time.h&gt; int main()&#123; srand((unsigned int)time(NULL)); // 生成伪随机数，每次的随机数都不一样 int n,m; unsigned long kb[700][700]; //默认最大棋盘大小 printf("请输入棋盘大小： "); scanf("%d",&amp;n); printf("请输入棋子数量： "); scanf("%d",&amp;m); while(m--) //生成 0 ~ n-1 的随机数 并赋值为 1 &#123; int a=rand()%n,b=rand()%n; kb[a][b]=1; &#125; for(int i=0;i&lt;n;i++) // 生 成 棋 盘 &#123; for(int j =0;j&lt;n;j++) // ┏┓┗┛┠┷┨┯┼● &#123; if(kb[i][j]==1) printf("●"); else &#123; if (i == 0 &amp;&amp; j == 0) printf("┏ "); else if (i == 0 &amp;&amp; j == n - 1) printf("┓ "); else if (i == n - 1 &amp;&amp; j == 0) printf("┗ "); else if (i == n - 1 &amp;&amp; j == n - 1) printf("┛ "); else if (j == 0) printf("┠ "); else if (i == n - 1) printf("┷ "); else if (j == n - 1) printf("┨ "); else if (i == 0) printf("┯ "); else printf("┼ "); &#125; &#125; putchar('\n'); &#125; int flag = 0; // 默认不是好棋。 for(int i=0;i&lt;n;i++) // 判断 好棋 坏棋 &#123; for(int j=0;j&lt;n;j++) &#123; if (kb[i][j] == 1) &#123; if (j&gt;0 &amp;&amp; kb[i][j-1] == 1) //判断同一行有无相邻棋子 &#123; printf("好棋！\n"); flag = 1; break; &#125; if (i &gt;0 &amp;&amp; kb[i-1][j] == 1) //判断同一列有无相邻棋子 &#123; printf("好棋！\n"); flag = 1; break; &#125; &#125; &#125; &#125; if(flag == 0) printf("不是好棋"); return 0; &#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++初学]]></title>
    <url>%2FAngelNI.github.io%2FC-%E5%88%9D%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[C++ 初学（一）在一次解决排序问题时，初步接触C++中的sort（）函数，在问题解决上非常好用，不用自己再写排序的代码，就像python中 import 函数库一样，因为懒嘛，所以更懒，嘿嘿！！所以想接触一下C++，简单学习一下子。 C++C++是C语言的继承，它既可以进行C语言的过程化程序设计，又可以进行以抽象数据类型为特点的基于对象的程序设计，还可以进行以继承和多态为特点的面向对象的程序设计。C++擅长面向对象程序设计的同时，还可以进行基于过程的程序设计，因而C++就适应的问题规模而论，大小由之。 C++ 是一种静态类型的、编译式的、通用的、大小写敏感的、不规则的编程语言，支持过程化编程、面向对象编程和泛型编程。 C++ 被认为是一种中级语言，它综合了高级语言和低级语言的特点。 C++ 是由 Bjarne Stroustrup 于 1979 年在新泽西州美利山贝尔实验室开始设计开发的。C++ 进一步扩充和完善了 C 语言，最初命名为带类的C，后来在 1983 年更名为 C++。 C++ 是 C 的一个超集，事实上，任何合法的 C 程序都是合法的 C++ 程序。 好了废话不多说，开始！ Hello world12345678910#include &lt;iostream&gt;using namespace std;// // main() 是程序开始执行的地方 int main()&#123; cout &lt;&lt; "Hello World"; // 输出 Hello World return 0;&#125; 接下来我们讲解一下上面这段程序： C++ 语言定义了一些头文件，这些头文件包含了程序中必需的或有用的信息。上面这段程序中，包含了头文件 。 下一行 using namespace std; 告诉编译器使用 std 命名空间。命名空间是 C++ 中一个相对新的概念。 下一行 // main() 是程序开始执行的地方 是一个单行注释。单行注释以 // 开头，在行末结束。 下一行 int main() 是主函数，程序从这里开始执行。 下一行 cout &lt;&lt; “Hello World”; 会在屏幕上显示消息 “Hello World”。 下一行 return 0; 终止 main( )函数，并向调用进程返回值 0。 输入、输出C++的输出和输入是用“流”(stream)的方式实现的｡有关流对象cin､cout和流运算符的定义等信息是存放在C++的输入输出流库中的,因此如果在程序中使用cin､cout和流运算符,就必须使用预处理命令把头文件stream包含到本文件中:#include &lt;iostream&gt;尽管cin和cout不是C++本身提供的语句,但是在不致混淆的情况下,为了叙述方便,常常把由cin和流提取运算符“&gt;&gt;”实现输入的语句称为输入语句或cin语句,把由cout和流插入运算符“&lt;&lt;”实现输出的语句称为输出语句或cout语句｡根据C++的语法,凡是能实现某种操作而且最后以分号结束的都是语句｡ 1234567891011121314151617181920212223#include&lt;cstdio&gt;#include&lt;iostream&gt;#include &lt;iomanip&gt; //格式化输出的头文件，注意这里不要加.husing namespace std;int main()&#123; int a,b,c; cin&gt;&gt;a&gt;&gt;b&gt;&gt;c; cout&lt;&lt;a&lt;&lt;setw(2)&lt;&lt;b&lt;&lt;setw(2)&lt;&lt;c&lt;&lt;endl; float num1 = 123.456f,num2 = 563.1f,num3 = 1.30f; float num4 = 123456.4444f; cout &lt;&lt; setprecision(4); cout &lt;&lt; "第一个数：" &lt;&lt; num1 &lt;&lt; endl; cout &lt;&lt; "第二个数：" &lt;&lt; num2 &lt;&lt; endl; cout &lt;&lt; "第三个数：" &lt;&lt; num3 &lt;&lt; endl; cout &lt;&lt; "第四个数：" &lt;&lt; num4 &lt;&lt; endl;//endl 英语意思是end of line,即一行输出结束，然后输出下一行。 return 0; &#125; 头文件#include&lt;iomanip&gt;是格式化输出的头文件，注意后面不加 .h， 使用setw()来控制占位宽度。注意事项 setw() 虽然带有括号，但是其实是一个操作符，并不是函数。 setw() 主要引用头文件 iomanip 才能使用。 如果setw() 所约束的输出超过了限制，不会被截断。是多少位就输出多少位。 如果输出是浮点数，小数点也会占一个位。 如果输出是字符串，空格有段有效字符，占一个位。从上面的输出结果也可以看出来。 setw() 只能约束住跟自己相邻的一个输出。也就是说 使用setprecision()控制浮点数有效位注意事项： setprecision() 同样是一个操作符，需要包含头文件 iomanip。 如果输出浮点数不足位，不会在其后面补0。 如果末尾有0，默认是不输出的。后面我们有其他方法可以输出末尾的0。 setprecision() 不同于setw()，setprecision() 设置之后，在下次设置之前都是有效的。从程序结果中可以看出来。 如果要输出的位数过多，则用科学计数法表示，10为基数。 setfioflags() 控制定点输出12345678910111213141516#include &lt;iostream&gt;#include &lt;iomanip&gt; using namespace std; int main()&#123; float num1 = 13.000f,num2 = 14.568f,num3 = 1.2f; cout &lt;&lt; setiosflags(ios::fixed|ios::showpoint); cout &lt;&lt; setprecision(4); cout &lt;&lt; "第一个数：" &lt;&lt; num1 &lt;&lt; endl; cout &lt;&lt; "第三个数：" &lt;&lt; num2 &lt;&lt; endl; cout &lt;&lt; "第二个数：" &lt;&lt; num3 &lt;&lt; endl; return 0; &#125; setiosflags() 是通过状态标志来实现对输出的控制的。状态标志功能如下表 状态标志 功能 ios::left 左对齐，右边填空格 ios::dec 所有整数以十进制输出 ios::right 右对齐，左边填空格 ios::scientific 以科学计数法形式输出浮点数 ios::hex 所有整数以十六进制输出 ios::fixed 以定点形式输出浮点数 ios::oct 所有整数以八进制输出 ios::showpoint 显示小数点和尾部的零 ios::showpos 在正数前面输出+ ios::uppercase 对于十六进制输出，使用大写字母表示 setiosflags() 需要与 setprecision() 一起使用。如果状态标志设为 ios::fixed，那么setprecision()设置的数字，就表示小数位的个数，不足补零。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Appreciation of Novels(1)]]></title>
    <url>%2FAngelNI.github.io%2FAppreciation%20of%20Novels%2F</url>
    <content type="text"><![CDATA[1.我因车祸而失明，所以我从不知女友长什么样。那年，她得了胃癌，临终前她将眼角膜移植给了我。我恢复光明后的第一件事就是找她的照片，然而我只找到她留给我的一封信，信里有一张空白照片，照片上写有一句话：“别再想我长什么样，下一个你爱上的人，就是我的模样。” 2.外婆离开人世的那个黄昏，外公在病房里陪伴着她走完了她生命的最后一段旅程。外婆临去前对外公说‘放学了’。一直假装平静的外公听完这句话后像个孩子似的大哭起来。葬礼结束后我问起外公这三个字的含义，外公告诉我说这是从前他和外婆还在上小学时外婆常说的一句话：放学了，我们一起回家吧。 3.她车祸去世后，他思念万分，利用时光机回到过去，阻止惨剧发生。机器出了差错，比预定时间早了几分钟。他拿出钥匙开门，听见卧室传出她的娇喘和男人的声音。她手机响了，他记得这是他打来的。“我得走了，我男人催我呢。”。他听着，惹羞成怒，出门偷了一辆车，看着急匆匆的她，一脚踩下油门。 4.妈妈你看！”小女孩开心地递过来一张写满字的纸。“我听见一个哥哥问姐姐怎么才会爱他，姐姐说只要每天在纸上写1000遍她的名字…”“傻孩子！”女人抱住小女孩：那宝贝是怎么知道爸爸名字的？“这里！”小女孩打开抽屉：上次爸爸把名字签在上面了！女人顺眼望去，里面躺着一张离婚协议书 5.他看着桌子上忙碌的蚂蚁，伸出手指，一下捏死一只。蚂蚁们大惊，四下乱窜。稍停，又排成一字继续工作。他又伸出手，再捏死一只。蚂蚁大乱，稍顷还是排一字。等到第10次时，蚂蚁们已经熟视无睹。当他向第11只下手时，轰隆一声，巨大的天花板砸了下来。他最后一眼，只看到推倒他房子的那只怪手。 6.他和她青梅竹马，相约为百姓杀贪官，仗剑天涯。一次刺杀失败被俘，他竟被招安，无数同仁被杀。她含泪发誓要刃叛徒，遂色诱贪官纳她为妾。十年后，他成平反大将。酒宴上，她起身献舞，刺中他手臂。他深情说，你之后，我再无爱过。她心软刃落，他抽刀刺死她，心想，真好骗。 7.他大她快二十岁，他对她很好，百般呵护，他们认识不到一年，他就执意要娶她。朋友都很羡慕她，她却犹豫不决，因为小时候一场手术意外造成她不孕，他是独子，庞大的家族事业等他继承，她不想耽误他。终於她鼓起勇气向他坦诚不孕的事实，他说我知道，当年那刀是我开的，这些年来我一直在找你！ 8.5岁“妈妈，烧红烧肉吧”“行，烧”15岁“妈妈，别烧红烧肉了，换换味道” “行，买别的菜” 35岁“儿子，啥时候回家吃一顿啊？妈给做红烧肉” “不行，最近忙” 50岁“妈妈今天路过你家，给你带红烧肉” “不行，今不在家” 70岁“妈，我想吃红烧肉” 那边，已经没有了妈妈的声音 9.就要做心脏移植手术了，他深情地望着躺在床上的妻子，拿签字表的手有点抖。“快签吧，你个窝囊废、穷鬼！”妻骂。手术很成功，她没有一点排异反应。“我那没心肝的丈夫哪？”她问护士。护士递过一张纸，上面画一颗鲜红的心和一行小字：“这是我最后能给你的了，我爱你。” 10.他与爸爸相依至大。他常问：为什么不给他找个后妈？爸爸总是笑说：此生只爱妈妈一个！后来他长大成家，爸爸说要结婚，他愤怒地打了那女人一耳光，骂爸爸是个骗子。从此，爸爸再未提及此事。多年后爸爸去世，他整理遗物时发现了一张自己婴儿时的照片，背面是沧桑的字迹：战友之子，当如吾儿！]]></content>
      <categories>
        <category>literature</category>
      </categories>
      <tags>
        <tag>literature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分法查找]]></title>
    <url>%2FAngelNI.github.io%2F%E4%BA%8C%E5%88%86%E6%B3%95%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[二分法查找二分法查找，二分搜所，也称折半搜索，每次查找区间减半，适用于数据量较大，对一个有序的数组中查找某一元素。 例如：给一有序的数组a[9]={1,2,3,4,5,6,7,8,9,}，想要确定 3 的位置。 实现：123(a[0]+a[8])/2=5 大于 3 则只需要查找a[0]~a[4]就可以(a[0]+a[4])/2=3 此时刚好等于3，则此时3的位置就是（0+4）/2=2则可知 a[2]=3 至此查找结束 下面通过一个例子来具体体验下二分法的妙处 Trailing Zeroesn的阶乘尾部有q个连续的0，现在给你q，请你算出满足条件的n，如果有多个n满足条件，输出最小的那个即可。 Input123输入一个T(T &lt;= 10000),表示样例数量。每个样例输入一个q。(1 &lt;= q &lt;= 100,000,000) output对于每个样例，输出满足条件的最小的n，如果没有满足条件的则输出”impossible”。. Sample Input12345673125 Sample Output12345Case 1: 5Case 2: 10Case 3: impossible 思路这是一个判断阶乘后面有多少个零，输出满足条件下的最小解。 首先判断0的个数，我们可以通过判断5的个数来判断0`的个数（10可以分成2*5） 1例如：5！=1*2*3*4*5=120 代码实现12345678910long long fn(long long n) //求n阶乘的末尾0个数 &#123; long long a = 0; while(n) &#123; a += n/5; n = n/5; &#125; return a;&#125; 例如：判断25！末尾有几个0 a=25/5 –&gt; a=5 a+=5/5 –&gt; a=6 由此可以判断25的阶乘末尾有6个零（拿计算器验证） 整个题解（这是大佬写的，我偷偷拿来哈~） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950##### #include&lt;iostream&gt; //cin，cout数据流输入输出的头文件#include&lt;cstdio&gt;using namespace std;typedef long long ll; //声明定义long long 的别名 llconst ll maxn = 1e17; //题目中0的个数 1~1e9ll fn(ll n)//求n阶乘的末尾0个数 &#123; ll a = 0; while(n) &#123; a += n/5; n = n/5; &#125; return a;&#125;int main()&#123; int n, q; ll ans;//定义所要求的答案 int Case = 0; cin&gt;&gt;n; //输入测试组数 while(n--) &#123; Case++;//判断测试第几个 cin&gt;&gt;q;//输入0的个数 int l, r;//定义左值，和右值 l =1; r = maxn; ans = 0; while(r&gt;=l) &#123; ll mid = (l+r)&gt;&gt;1; //直接平均可能溢出，所以用这个 注： &gt;&gt; 值的二进制形式右移一位，相当于十进制除2 if(fn(mid)==q) &#123; ans = mid;//如果中间的那个数零的个数恰好等于q，则为答案 r = mid-1; &#125; else if(fn(mid)&lt;q) l = mid+1;//如果中间的值0的个数小于q，则左值++ else r = mid-1;// 否则 右值—— &#125; if(ans) printf("Case %d: %lld", Case, ans);//如果结果不为零，按输出格式打印 else printf("Case %d: impossible", Case);//否则，则输出impossile cout&lt;&lt;endl; &#125; return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Makedown]]></title>
    <url>%2FAngelNI.github.io%2FMakedown%2F</url>
    <content type="text"><![CDATA[最近经过几天的奋斗，自己的博客基本框架终于搭建成功，接下来就是开始写文章了，不过上来就碰上个头疼的问题，就是怎么写的问题，我就想度娘请教（度娘，真帅），说是用Markdown语法写，最近在Notebook上写Python的学习笔记，老师说过要自学Markdown语法，说是非常简单（没错，就是很简单），下面就是Markdown的语法介绍啦，要好好学哟。 在编写 Markdown 时，强烈的推荐给大家一款简洁易用的 Markdown 编辑器 —— Typora 按照官方的说法就是 简单而强大，它不仅支持原生的语法，也支持对应的快捷键，更重要的是它还可以 实时预览 这里附上 Typora 的下载地址：点击这里 , 有兴趣的朋友可以下载来试试 好，下面开始进入正题，介绍一些常用的 Markdown 语法 (1) 标题Markdown语法：1234567891011#一级标题``##二级标题``###三级标题``####四级标题``#####五级标题``######六级标题 Typora快捷键：1234567891011Ctrl + 1：一级标题Ctrl + 2：二级标题Ctrl + 3：三级标题Ctrl + 4：四级标题Ctrl + 5：五级标题Ctrl + 6：六级标题 (2)粗体、斜体、删除线Markdown语法：1234567*斜体*``**粗体**``***加粗斜线***``~~删除线~~ Typora快捷键：1234567Ctrl+l ：斜体Ctrl+B：粗体Ctrl+U：下划线Alt + Shift + 5 ：删除线 (3) 引用块Markdown语法：1&gt; 文字引用 Typora快捷键：1Ctrl + Shift + Q (4)代码块Markdown语法：12&apos; 行内代码&apos;&apos;&apos;&apos; 多行代码&apos;&apos;&apos; Typora快捷键：123行内代码： Ctrl + Shift + `多行代码：Ctrl + Shift +K (5)公式块Markdown语法：1$$数学公式$$ Typora快捷键：1Ctrl + Shift + M (6)分割线Markdown 语法：12345方法1：---``方法2：+++``方法3：*** (7)列表Markdown语法：12341.有序列表项2. * 无序列表项3. + 无序列表项4. - 无序列表项 Typora快捷键：有序列表项：Ctrl+Shift+[ 有序列表项：Ctrl+Shift+] (8) 表格Markdown语法：12341. 表头1|表头2-|-|-内容11|内容12内容21|内容22 Typora 快捷键：Ctrl+T (9)超链接Markdown语法：1234方法一：[链接文字](链接地址 &quot;链接描述&quot;)例如：[示例链接](https://www.example.com/ &quot;示例链接&quot;)方法二：&lt;链接地址&gt;例如：&lt;https://www.example.com/&gt; Typora快捷键：Ctrl+K (10)图片Markdown语法：12![图片文字](图片地址 &quot;图片描述&quot;)例如：![示例图片](https://www.example.com/example.PNG &quot;示例图片&quot;) Typora快捷键：Ctrl+Shift+I]]></content>
      <categories>
        <category>Makedown</category>
      </categories>
      <tags>
        <tag>Makedown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github]]></title>
    <url>%2FAngelNI.github.io%2Fhexo-github%2F</url>
    <content type="text"><![CDATA[博客搭建(自己总结)之前自己搭建博客，可以说废了很大的劲，这里总结了一下各位大佬们搭建博客的方法，供大家借鉴。 1.安装所需软件1.git安装Windows系统下安装git 可以直接到官网下载安装点击这里 由于访问的是外网，下载速度可能会慢（也可能非常慢） 下面提供百度云的下载地址 64-bit Git for Windows Setup : https://npm.taobao.org/mirrors/git-for-windows/v2.21.0.windows.1/Git-2.21.0-64-bit.exe 2.Node.js安装可以直接到官网下载安装点击这里 百度网盘资源： https://pan.baidu.com/s/1hKVcYfPorRX89hl7D4R1eA 提取码：wsti 下载完成后，安装时一定要点击 Add to PATH 安装完成后，打开cmd，输入 node -v 测试安装是否成功 下面来解决npm卡顿问题 1.打开cmd，换成阿里源 11.npm config set registry https://registry.npm.taobao.org 2.验证命令 12.npm config get registry //返回https://registry.npm.taobao.org，说明镜像配置成功 3.安装cnpm 13.npm install -g cnpm --registry=https://registry.npm.taobao.org 3.hexo安装打开git目录下的git-bash.exe，输入下面代码 npm install -g hexo-cli 安装hexo完成后执行下列命令 123hexo init &lt;文件夹名&gt; cd &lt;文件夹名&gt;npm install hexo 理论上安装在git文件夹下 4.在github上注册账号并同时建立仓库gitHub是一个面向开源及私有软件项目的托管平台，因为只支持git 作为唯一的版本库格式进行托管，故名gitHub。（来源百度百科） github官网点击这里 这个是github基础设置和使用详解点击这里 5.ssh授权获取私钥先配置SSH,在git-bash下输入 12git config --global user.name &quot;github注册名&quot;git config --global user.email &quot;github注册邮箱&quot; 打开git bash，输入ssh-kengen -t rsa，停顿时，敲击回车 最后会在C盘目录下生成id_ras和id_rsa.pub两个文件夹，用记事本打开id_rsa.pub，复制打开的文件内容到 github-&gt;setting-&gt;SSH and GPG key 下 添加后，在git-bash进行测试，输入 ssh -T git@github.com 如果返回Hi username ！You’ve successfully ……，说明配置成功 6.配置_config.yml打开你的hexo目录下的_config.yml文档（我用的是notepad++打开的） 修改最下面的deploy下的内容 12345type: gitrepository : //这里是你的仓库下，点击Clone ordownload（绿色的）点击Use SSH复制框框内的内容到这里。branch：//这里是你的bransh名称，默认为master 下面来修改 url和 root 123url ：// 是你的github 分配的地址root：// 是你的仓库的名字 一定要注意每一项冒号后有一个英文空格 7.本地测试打开git bash进入博客的根目录（cd + 文件夹名） 输入 12345hexo cleanhexo ghexo s hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容。 显示的主题是 hexo 默认的 hexo 操作指令点击这里 8.上传到github仓库首先先安装hexo拓展库，打开git bash输入 12npm install hexo-deployer-git --savenpm install 然后输入 1234hexo clean//清除缓存hexo g//生成静态文件hexo d/上传打开github分配的网站，就可以看到你的blog了 后记自己搭建博客可能不是一帆风顺的，可能遇到各种不同的错误，一定要耐得住性子，一步一步来搭建。 记得，一定要善用搜索，遇到不懂得问题去百度上搜索。 最后，度娘，可真帅哪！！！]]></content>
      <categories>
        <category>hexo+github</category>
      </categories>
      <tags>
        <tag>hexo+github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello world]]></title>
    <url>%2FAngelNI.github.io%2FHello-world-0%2F</url>
    <content type="text"><![CDATA[记一次过程经历 最近，自己在搭一个博客，用 hexo+github，一开始按照网上的教程下载相应的软件，cmd安装，git bash 安装hexo，最开始可是在自己的本地可以预览，效果不错，直接就上传到github上的我的repo，结果cmd出错，由于对hexo 文件的位置不清，和出现的错误，我直接就删除了hexo，但是不是直接的卸载，有些文件还在（找不到啊 呜呜~），我用重新npm安装，发现还是不行，当时头大到想要格式化电脑了（还好没有，要不然就要重新慢慢下载了），然后我去找baidu，简书，发现自己的许多错误操作。好吧，我又从新开始。还是有error，我就按个所错误，发现自己有许多的本地配置没有设置，我就一条一条的修改（可气人了，一步踩一个坑，一步填一个坑），花了两天才把本地配置好（上课没时间啊）。 又因为，默认的主题很low（只是个人而言），不怎么喜欢，想换个主题，hexo上面有好多的主题，一个一个翻看，也没有喜欢的，现在比较流行的是NEXT主题（没错我找了度娘），就在网上下载NEXT主题包，一开始的主题内容，没设计的，所以我又找了度娘，还在CSDN上搜搜，发现许多大佬掉了许多的坑，也有填坑的方法(坑和经验，傻傻分不清），就开始一步一步自己搭，终于，经过一天多的自己的骚操作，在清明节的下午本地可以看到比较好的主题设置（还有点不满意，后续会修改）。 感受 （真是不作死不会死，谁TM知道我为什么突然想到建博客呢！） 经过三天多自己的摸索，我这个小小白终于把自己的blog建成了（~~热烈欢迎大家访问）。在建博客的过程中，真是有点头大了，因为自己对命令行操作一点不了解，完全是两眼一模黑，在黑暗中摸索（哈哈，有点夸张），出现错误也不知道哪里的问题，真的很感谢度娘的帮助（说白了，多谢大佬们的坑啊）。 这篇文章的标题是printf(“Hello,world”),学C语言的都知道这是入门的基础程序，我想用这个标题想说，当你去接触一个陌生不懂的东西时，不是一帆风顺的，总会有大波小浪，不过，经过自己的一步一步的摸索总会找到自己的___（你懂的哈），不管如何都不要放弃，既然已经迈开了重要的第一步，就要坚持走到终点。 对了，还有还有，“生命在于运动，电脑在于折腾啊”，不管会不会，先折腾折腾吧（坏了我可不管~~) . . . . . . . . 度娘，真帅！！！]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
</search>
